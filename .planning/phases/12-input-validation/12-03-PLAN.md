---
phase: 12-input-validation
plan: 03
type: execute
wave: 3
depends_on: ["12-02"]
files_modified:
  - lib/agent_com/dashboard_state.ex
  - lib/agent_com/dashboard.ex
  - test/agent_com/validation_test.exs
  - test/agent_com/validation/violation_tracker_test.exs
autonomous: true
must_haves:
  truths:
    - "Dashboard shows validation error counts per agent"
    - "Dashboard shows recent validation failures with message type and error details"
    - "Dashboard shows agents disconnected for validation violations"
    - "Tests cover all 15 WebSocket message type validations"
    - "Tests cover HTTP endpoint validations"
    - "Tests cover violation tracking threshold and backoff logic"
    - "All existing tests pass alongside new validation tests"
  artifacts:
    - path: "lib/agent_com/dashboard_state.ex"
      provides: "Validation metrics in dashboard snapshot"
      contains: "validation"
    - path: "lib/agent_com/dashboard.ex"
      provides: "Validation health card in HTML dashboard"
      contains: "Validation"
    - path: "test/agent_com/validation_test.exs"
      provides: "Comprehensive validation module tests"
      min_lines: 100
    - path: "test/agent_com/validation/violation_tracker_test.exs"
      provides: "Violation tracker unit tests"
      min_lines: 40
  key_links:
    - from: "lib/agent_com/dashboard_state.ex"
      to: "PubSub validation topic"
      via: "subscribe to validation events"
      pattern: "subscribe.*validation"
    - from: "lib/agent_com/dashboard.ex"
      to: "lib/agent_com/dashboard_state.ex"
      via: "snapshot includes validation metrics"
      pattern: "validation"
---

<objective>
Add validation metrics to the dashboard (LOCKED decision: dashboard visibility) and comprehensive test coverage for the validation system.

Purpose: Dashboard visibility of validation errors is a locked user decision. Tests validate that all 15 message types, all HTTP endpoints, and the violation tracking system work correctly. This completes Phase 12 requirements.

Output: Modified dashboard with validation card, comprehensive test suite.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-input-validation/12-CONTEXT.md
@.planning/phases/12-input-validation/12-01-SUMMARY.md
@.planning/phases/12-input-validation/12-02-SUMMARY.md
@lib/agent_com/dashboard_state.ex
@lib/agent_com/dashboard.ex
@lib/agent_com/validation.ex
@lib/agent_com/validation/schemas.ex
@lib/agent_com/validation/violation_tracker.ex
@lib/agent_com/socket.ex
@lib/agent_com/endpoint.ex
@test/test_helper.exs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add validation metrics to DashboardState and Dashboard</name>
  <files>lib/agent_com/dashboard_state.ex, lib/agent_com/dashboard.ex</files>
  <action>
**Modify `lib/agent_com/dashboard_state.ex`:**

1. Subscribe to the "validation" PubSub topic in `init/1`:
```elixir
Phoenix.PubSub.subscribe(AgentCom.PubSub, "validation")
```

2. Add validation tracking fields to init state:
```elixir
validation_failures: [],          # Ring buffer of recent failures (cap 50)
validation_failure_counts: %{},   # agent_id => count (this hour)
validation_disconnects: []        # List of %{agent_id, timestamp} for recent disconnects
```

3. Add `handle_info` clauses for validation events:

```elixir
def handle_info({:validation_failure, info}, state) do
  # Add to recent failures ring buffer (cap 50)
  entry = %{
    agent_id: info.agent_id,
    message_type: info.message_type,
    error_count: info.error_count,
    timestamp: info.timestamp
  }
  failures = [entry | state.validation_failures] |> Enum.take(50)

  # Increment per-agent count
  agent_id = info.agent_id || "unknown"
  counts = Map.update(state.validation_failure_counts, agent_id, 1, &(&1 + 1))

  {:noreply, %{state | validation_failures: failures, validation_failure_counts: counts}}
end

def handle_info({:validation_disconnect, info}, state) do
  entry = %{agent_id: info.agent_id, timestamp: info.timestamp}
  disconnects = [entry | state.validation_disconnects] |> Enum.take(20)
  {:noreply, %{state | validation_disconnects: disconnects}}
end
```

4. Include validation metrics in the snapshot (in `handle_call(:snapshot, ...)`):

Add to the snapshot map:
```elixir
validation: %{
  recent_failures: state.validation_failures,
  failure_counts_by_agent: state.validation_failure_counts,
  recent_disconnects: state.validation_disconnects,
  total_failures_this_hour: state.validation_failure_counts |> Map.values() |> Enum.sum()
}
```

5. Reset validation_failure_counts in the `:reset_hourly` handler:
```elixir
validation_failure_counts: %{}
```

6. Add validation to health conditions in `compute_health/3`:

If total_failures_this_hour > 50, add a WARNING condition:
```elixir
# 7. High validation failure rate
total_val_failures = state.validation_failure_counts |> Map.values() |> Enum.sum()
conditions = if total_val_failures > 50 do
  ["High validation failures: #{total_val_failures} this hour" | conditions]
else
  conditions
end
```

**Modify `lib/agent_com/dashboard.ex`:**

Add a "Validation Health" card to the HTML dashboard, after the DETS Health card. The card should display:

1. **Total failures this hour** -- number from `validation.total_failures_this_hour`
2. **Failures by agent** -- table with agent_id and count columns, from `validation.failure_counts_by_agent`
3. **Recent disconnects** -- list of agent_id + timestamp for agents disconnected due to validation violations
4. **Recent failures** -- last 10 validation failures showing agent_id, message_type, timestamp

The card should be styled consistently with existing dashboard cards. Use the same pattern as the DETS Health card for the HTML structure.

In the JavaScript section, add rendering for the validation data when the WebSocket pushes state updates. The validation data comes from the snapshot's `validation` key.

Card HTML structure:
```html
<div class="card" id="validation-card">
  <h3>Validation Health</h3>
  <div class="stat">Failures this hour: <span id="val-total">0</span></div>
  <div id="val-by-agent"></div>
  <div id="val-disconnects"></div>
  <div id="val-recent"></div>
</div>
```

Keep the implementation minimal -- just render the data. No charts, no animations (consistent with existing dashboard style).
  </action>
  <verify>
Run `mix compile --warnings-as-errors` -- compiles cleanly. Run `mix test` -- existing tests pass. Start the hub with `mix run --no-halt` and visit http://localhost:4000/dashboard -- verify the Validation Health card appears. Send some invalid WebSocket messages to generate validation failures and verify the card updates with failure counts.
  </verify>
  <done>
Dashboard shows validation error counts per agent, recent failures with message type details, and disconnected-for-violations agents. DashboardState subscribes to validation PubSub topic and includes validation metrics in snapshot. Health conditions include high validation failure rate warning.
  </done>
</task>

<task type="auto">
  <name>Task 2: Comprehensive validation test suite</name>
  <files>test/agent_com/validation_test.exs, test/agent_com/validation/violation_tracker_test.exs</files>
  <action>
Create comprehensive tests following existing test patterns (see test/agent_com/*_test.exs for style).

**Create `test/agent_com/validation_test.exs`:**

```elixir
defmodule AgentCom.ValidationTest do
  use ExUnit.Case, async: true

  alias AgentCom.Validation
  alias AgentCom.Validation.Schemas
```

Test groups:

**1. WebSocket message validation -- valid messages (one test per message type):**
- `test "validates identify with all required fields"` -- `%{"type" => "identify", "agent_id" => "a1", "token" => "tok"}` returns `{:ok, _}`
- `test "validates identify with optional fields"` -- includes name, status, capabilities
- `test "validates message with required fields"` -- `%{"type" => "message", "payload" => %{"text" => "hi"}}`
- `test "validates status"` -- `%{"type" => "status", "status" => "working"}`
- `test "validates list_agents"` -- `%{"type" => "list_agents"}`
- `test "validates ping"` -- `%{"type" => "ping"}`
- `test "validates channel_subscribe"` -- with channel field
- `test "validates channel_unsubscribe"` -- with channel field
- `test "validates channel_publish"` -- with channel and payload
- `test "validates channel_history"` -- required channel, optional limit/since
- `test "validates list_channels"` -- type only
- `test "validates task_accepted"` -- with task_id
- `test "validates task_progress"` -- with task_id, optional progress
- `test "validates task_complete"` -- with task_id and generation (integer)
- `test "validates task_failed"` -- with task_id and generation (integer)
- `test "validates task_recovering"` -- with task_id

**2. WebSocket validation -- missing required fields:**
- `test "rejects identify missing agent_id"` -- returns error with field "agent_id", error :required
- `test "rejects identify missing token"` -- returns error with field "token"
- `test "rejects message missing payload"` -- returns error for payload
- `test "rejects channel_subscribe missing channel"` -- error for channel
- `test "rejects task_complete missing generation"` -- error for generation
- `test "rejects task_complete missing task_id"` -- error for task_id
- `test "returns multiple errors for multiple missing fields"` -- identify with only type returns errors for both agent_id and token

**3. WebSocket validation -- wrong types (strict, no coercion):**
- `test "rejects string where integer expected"` -- `%{"type" => "task_complete", "task_id" => "t1", "generation" => "not_int"}` returns :wrong_type with value echoed
- `test "rejects integer where string expected"` -- `%{"type" => "status", "status" => 42}` returns :wrong_type
- `test "rejects string where map expected"` -- `%{"type" => "message", "payload" => "not_a_map"}`
- `test "rejects integer where list expected"` -- capabilities as integer

**4. WebSocket validation -- unknown type and missing type:**
- `test "rejects unknown message type"` -- returns :unknown_message_type with known_types list
- `test "rejects missing type field"` -- returns :required for type
- `test "rejects non-string type"` -- `%{"type" => 123}` returns :wrong_type

**5. WebSocket validation -- unknown fields pass through:**
- `test "passes through unknown fields"` -- `%{"type" => "ping", "extra_field" => "value"}` returns `{:ok, _}` with extra_field preserved
- `test "passes through deeply nested unknown fields"` -- payload with arbitrary structure

**6. String length limits:**
- `test "rejects agent_id over 128 chars"` -- long agent_id in identify
- `test "rejects channel over 64 chars"` -- long channel name
- `test "accepts agent_id at limit"` -- exactly 128 chars

**7. HTTP validation:**
- `test "validates post_task with required description"` -- `{:ok, _}`
- `test "rejects post_task missing description"` -- `{:error, _}`
- `test "validates post_task with optional fields"` -- priority, metadata, etc.
- `test "validates put_heartbeat_interval"` -- positive integer required
- `test "rejects put_heartbeat_interval with zero"` -- 0 is not positive
- `test "rejects put_heartbeat_interval with string"` -- wrong type
- `test "validates post_channel with name"` -- `{:ok, _}`
- `test "validates post_onboard_register with agent_id"` -- non-empty string
- `test "rejects post_onboard_register with empty agent_id"` -- empty string fails
- `test "validates post_mailbox_ack with seq"` -- integer seq

**8. Schema introspection:**
- `test "known_types returns all 15 message types"` -- exactly 15 types
- `test "to_json returns JSON-serializable structure"` -- can Jason.encode the result
- `test "all schemas have required and optional maps"` -- structural check
- `test "http_schema returns schemas for known keys"` -- all defined keys work

**9. Error format:**
- `test "format_errors converts atoms to strings"` -- error atom becomes string
- `test "format_errors includes value when present"` -- :wrong_type errors have value
- `test "format_errors omits value when nil"` -- :required errors have no value

**Create `test/agent_com/validation/violation_tracker_test.exs`:**

```elixir
defmodule AgentCom.Validation.ViolationTrackerTest do
  use ExUnit.Case, async: false  # ETS operations

  alias AgentCom.Validation.ViolationTracker
```

Test groups:

**1. Per-connection violation tracking:**
- `test "track_violation increments count"` -- starts at 0, returns state with count 1
- `test "track_violation resets after window expires"` -- set window_start to > 60s ago, count resets to 1
- `test "should_disconnect? returns false under threshold"` -- count 9
- `test "should_disconnect? returns true at threshold"` -- count 10

**2. Cross-connection backoff (ETS):**
- `test "check_backoff returns :ok for unknown agent"` -- no entry
- `test "record_disconnect creates entry"` -- first disconnect
- `test "check_backoff returns cooldown after disconnect"` -- 30s for first disconnect
- `test "backoff escalates on repeated disconnects"` -- 2nd = 60s, 3rd = 300s
- `test "clear_backoff removes entry"` -- back to :ok

Setup: Each test should use a unique agent_id to avoid cross-test interference. Add `setup` that clears the test agent from ETS after each test.

IMPORTANT: Since the ETS table `:validation_backoff` is created in application.ex, it exists during test runs. If for some reason it doesn't (test env doesn't start app), create it in the test setup with `try do :ets.new(:validation_backoff, [:named_table, :public, :set]) catch :error, :badarg -> :ok end`.
  </action>
  <verify>
Run `mix test test/agent_com/validation_test.exs test/agent_com/validation/violation_tracker_test.exs --trace` -- all tests pass. Run `mix test` -- full suite passes including new and existing tests. Verify test count increased.
  </verify>
  <done>
Comprehensive test suite covers all 15 WebSocket message types (valid, missing required, wrong type), all HTTP endpoint schemas, unknown field pass-through, string length limits, schema introspection, error formatting, per-connection violation tracking, and cross-connection backoff escalation. All existing tests continue to pass.
  </done>
</task>

</tasks>

<verification>
- `mix compile --warnings-as-errors` passes
- `mix test` passes (all existing + new tests)
- Dashboard at /dashboard shows Validation Health card
- DashboardState includes validation metrics in snapshot
- Test coverage: all 15 WS types validated, all HTTP schemas validated, violation tracking tested
</verification>

<success_criteria>
- Dashboard displays validation error counts per agent, recent failures, and disconnected agents
- Tests cover every WebSocket message type (valid + invalid cases)
- Tests cover every HTTP endpoint schema
- Tests cover violation tracking threshold (10 in 1 min) and backoff escalation (30s, 1m, 5m)
- Full test suite passes with zero failures
</success_criteria>

<output>
After completion, create `.planning/phases/12-input-validation/12-03-SUMMARY.md`
</output>
