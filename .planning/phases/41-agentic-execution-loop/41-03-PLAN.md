---
phase: 41-agentic-execution-loop
plan: 03
type: execute
wave: 2
depends_on: ["41-01"]
files_modified:
  - sidecar/lib/execution/agentic-prompt.js
  - sidecar/lib/execution/progress-emitter.js
  - sidecar/lib/execution/verification-loop.js
  - sidecar/test/execution/agentic-prompt.test.js
autonomous: true

must_haves:
  truths:
    - "Agentic system prompt describes available tools with usage patterns and few-shot examples for Qwen3 8B"
    - "Dashboard receives real-time tool_call events during agentic execution via WebSocket"
    - "Task that times out or exhausts iterations preserves partial work and reports partial_pass with termination reason"
  artifacts:
    - path: "sidecar/lib/execution/agentic-prompt.js"
      provides: "System prompt builder with few-shot examples"
      exports: ["buildAgenticSystemPrompt"]
    - path: "sidecar/test/execution/agentic-prompt.test.js"
      provides: "Prompt builder tests"
  key_links:
    - from: "sidecar/lib/execution/ollama-executor.js"
      to: "sidecar/lib/execution/agentic-prompt.js"
      via: "buildAgenticSystemPrompt() import for _agenticLoop"
      pattern: "require.*agentic-prompt"
    - from: "sidecar/lib/execution/progress-emitter.js"
      to: "sidecar/index.js"
      via: "tool_call events flow through ProgressEmitter to WebSocket"
      pattern: "type.*tool_call"
---

<objective>
Build the agentic system prompt with few-shot examples, wire tool_call progress events to the dashboard, and handle partial results on forced termination.

Purpose: Complete the agentic execution pipeline by giving the LLM proper instructions (system prompt), giving the user real-time visibility (dashboard streaming), and preserving work on timeout (partial results).

Output: agentic-prompt.js module, updated progress-emitter.js and verification-loop.js, prompt tests.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/41-agentic-execution-loop/41-CONTEXT.md
@.planning/phases/41-agentic-execution-loop/41-RESEARCH.md
@.planning/phases/41-agentic-execution-loop/41-01-SUMMARY.md
@sidecar/lib/execution/ollama-executor.js
@sidecar/lib/execution/progress-emitter.js
@sidecar/lib/execution/verification-loop.js
@sidecar/index.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Agentic system prompt builder with few-shot examples</name>
  <files>sidecar/lib/execution/agentic-prompt.js, sidecar/test/execution/agentic-prompt.test.js, sidecar/lib/execution/ollama-executor.js</files>
  <action>
Create `sidecar/lib/execution/agentic-prompt.js` exporting `buildAgenticSystemPrompt(task, toolDefinitions)`.

The function returns a string containing the system prompt for agentic execution. Structure:

**Section 1: Role and context**
```
You are a coding agent. You complete tasks by reading files, understanding code, making changes, and verifying your work.

Repository: {task.repo || 'unknown'}
Branch: {task.branch || 'main'}
{if task.file_hints: "Key files: " + task.file_hints.join(', ')}
```

**Section 2: Available tools**
For each tool in toolDefinitions, list name, description, and required parameters. Format as a concise reference:
```
## Available Tools

### read_file
Read file contents. Args: path (required), start_line, end_line

### write_file
Write content to file. Args: path (required), content (required), create_dirs

### list_directory
List files/dirs. Args: path (required), recursive, pattern

### run_command
Execute shell command. Args: command (required), timeout_ms

### search_files
Search for regex pattern. Args: pattern (required), path, file_pattern, max_results
```

**Section 3: Workflow pattern**
```
## How to Complete Tasks

Follow this pattern:
1. UNDERSTAND: Read relevant files to understand the codebase
2. PLAN: Identify what needs to change (think step by step)
3. IMPLEMENT: Make changes using write_file
4. VERIFY: Run tests or commands to check your work

Always read before writing. Never guess at file contents.
```

**Section 4: Few-shot example** (per locked decision -- Qwen3 8B benefits from concrete examples)
Include one concrete example of a multi-turn tool-calling interaction:
```
## Example

Task: "Add a greeting function to utils.js"

Step 1 - Read the file:
Call read_file with path: "src/utils.js"

Step 2 - Understand current content, then write the update:
Call write_file with path: "src/utils.js", content: [updated content with new function]

Step 3 - Verify:
Call run_command with command: "node -e \"require('./src/utils').greeting('world')\""

Step 4 - Report completion:
"Done. Added greeting() function to src/utils.js. Verified it works by running it."
```

**Section 5: Success criteria** (if task has them)
```
{if task.success_criteria: "## Success Criteria\n" + task.success_criteria.map(c => "- " + c).join('\n')}
```

**Section 6: Rules**
```
## Rules
- Call tools to do work. Do not just describe what you would do.
- When done, provide a brief summary of what you accomplished.
- If you encounter an error, try to fix it. If stuck, explain what went wrong.
```

**Wire into OllamaExecutor:**
In `_agenticLoop()`, replace the basic system prompt with:
```javascript
const { buildAgenticSystemPrompt } = require('./agentic-prompt');
const systemPrompt = buildAgenticSystemPrompt(task, tools);
```
Use this as the system message content in the initial messages array.

**Tests:** Create `sidecar/test/execution/agentic-prompt.test.js`:
- Contains repo name when task.repo is set
- Contains branch when task.branch is set
- Contains file hints when present
- Lists all 5 tool names
- Contains few-shot example section
- Contains success criteria when task has them
- Omits success criteria section when task has none
- Contains workflow pattern (UNDERSTAND, PLAN, IMPLEMENT, VERIFY)
  </action>
  <verify>Run `cd sidecar && npx jest test/execution/agentic-prompt.test.js --verbose` -- all tests pass.</verify>
  <done>buildAgenticSystemPrompt() creates a structured prompt with tool descriptions, workflow pattern, few-shot example, and success criteria. Wired into OllamaExecutor._agenticLoop().</done>
</task>

<task type="auto">
  <name>Task 2: Dashboard streaming and partial results</name>
  <files>sidecar/lib/execution/progress-emitter.js, sidecar/lib/execution/verification-loop.js</files>
  <action>
**Part A: Progress emitter tool_call event support (AGENT-07)**

Update `sidecar/lib/execution/progress-emitter.js`:
The ProgressEmitter already passes non-token events through immediately (line 40-42). The `tool_call` event type will flow through this existing path with no code change needed in ProgressEmitter itself.

However, verify the WebSocket handler in `sidecar/index.js` (around line 870-882) correctly passes tool_call events. The existing code extracts `event.type` as `event_type`, `event.text || event.message` as `text`. For tool_call events, we need the handler to also pass through the additional fields.

Update the ProgressEmitter flush handler in `sidecar/index.js` (around line 870) to include tool_call-specific fields:
```javascript
const emitter = new ProgressEmitter((events) => {
  for (const event of events) {
    const execution_event = {
      event_type: event.type,
      text: event.text || event.message || '',
      tokens_so_far: event.tokens_so_far || null,
      model: event.model || null,
      timestamp: Date.now()
    };
    // Include tool_call fields when present
    if (event.type === 'tool_call') {
      execution_event.tool_name = event.tool_name || null;
      execution_event.args_summary = event.args_summary || null;
      execution_event.result_summary = event.result_summary || null;
      execution_event.iteration = event.iteration || null;
    }
    this.send({
      type: 'task_progress',
      task_id: task.task_id,
      execution_event
    });
  }
}, { batchIntervalMs: 100 });
```

Also, in `_agenticLoop()` in ollama-executor.js, after executing each tool, emit a result_summary in the progress event. Add a helper `_summarizeResult(toolResult)` that returns a short string:
- Success: `"OK: {tool} -> {brief output}"` (truncate output to 100 chars)
- Failure: `"ERR: {tool} -> {error.message}"` (truncate to 100 chars)

**Part B: Partial results handling (AGENT-09)**

Update `sidecar/lib/execution/verification-loop.js` to handle agentic partial results:

When `executeWithVerification()` receives a result with `termination_reason` not equal to `'final_answer'` (meaning the loop was force-stopped), and the result status is still `'success'`:
1. The verification loop should still run verification on the current workspace state
2. If some checks pass but not all, report `partial_pass`
3. Include the `termination_reason` and `iterations_used` in the loop result

Add these fields to `buildLoopResult()`:
```javascript
function buildLoopResult(execResult, reports, cumulativeCost, status) {
  return {
    ...execResult,
    // ... existing fields ...
    termination_reason: execResult.termination_reason || null,
    iterations_used: execResult.iterations_used || null
  };
}
```

This way the hub receives the termination context and can decide whether to create a follow-up task.
  </action>
  <verify>
Verify modules load: `cd sidecar && node -e "require('./lib/execution/progress-emitter'); require('./lib/execution/verification-loop'); console.log('OK')"`.
Run existing tests: `cd sidecar && npx jest --verbose 2>&1 | tail -20`.
  </verify>
  <done>Tool_call events stream through ProgressEmitter to WebSocket with tool_name, args_summary, result_summary. Partial results from forced termination include termination_reason and iterations_used. Verification loop handles agentic results gracefully.</done>
</task>

</tasks>

<verification>
1. `cd sidecar && npx jest test/execution/agentic-prompt.test.js --verbose` -- prompt tests pass
2. `cd sidecar && npx jest --verbose` -- all existing tests pass
3. `node -e "require('./sidecar/lib/execution/agentic-prompt')"` -- no import errors
4. `node -e "require('./sidecar/lib/execution/verification-loop')"` -- no import errors
</verification>

<success_criteria>
- System prompt includes tool descriptions, workflow pattern, few-shot example, and task-specific context per AGENT-10
- Dashboard receives tool_call events with tool_name, args_summary, result_summary, iteration per AGENT-07
- Forced termination (timeout, max iterations, repetition) preserves partial work per AGENT-09
- Verification runs on current workspace state after forced termination
- termination_reason and iterations_used flow through to hub in task result
</success_criteria>

<output>
After completion, create `.planning/phases/41-agentic-execution-loop/41-03-SUMMARY.md`
</output>
