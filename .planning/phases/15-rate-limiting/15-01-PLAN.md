---
phase: 15-rate-limiting
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - lib/agent_com/rate_limiter.ex
  - lib/agent_com/rate_limiter/config.ex
  - lib/agent_com/application.ex
  - test/agent_com/rate_limiter_test.exs
autonomous: true

must_haves:
  truths:
    - "RateLimiter.check/3 returns {:allow, remaining} for an agent's first request"
    - "RateLimiter.check/3 returns {:deny, retry_after_ms} when tokens are exhausted"
    - "RateLimiter.check/3 returns {:warn, remaining} when usage crosses 80% threshold"
    - "Tokens refill over time via lazy refill -- an agent that waits regains capacity"
    - "Config.ws_tier/1 classifies all known WS message types into light/normal/heavy"
    - "Config.http_tier/1 classifies all known HTTP actions into light/normal/heavy"
    - "Exempt agents always get {:allow, :exempt} regardless of usage"
    - "Progressive backoff escalates retry_after_ms for consecutive violations"
    - "Violation count resets after a quiet period with no violations"
  artifacts:
    - path: "lib/agent_com/rate_limiter.ex"
      provides: "Token bucket core with check/3, record_violation/1, rate_limited?/1, exempt?/1"
      contains: "defmodule AgentCom.RateLimiter"
    - path: "lib/agent_com/rate_limiter/config.ex"
      provides: "Tier classification and default thresholds"
      contains: "defmodule AgentCom.RateLimiter.Config"
    - path: "lib/agent_com/application.ex"
      provides: "ETS table creation for :rate_limit_buckets and :rate_limit_overrides"
      contains: ":rate_limit_buckets"
    - path: "test/agent_com/rate_limiter_test.exs"
      provides: "TDD tests for token bucket, tier classification, backoff, exemption"
      contains: "defmodule AgentCom.RateLimiterTest"
  key_links:
    - from: "lib/agent_com/rate_limiter.ex"
      to: ":rate_limit_buckets ETS"
      via: ":ets.lookup and :ets.insert"
      pattern: "ets\\.(lookup|insert).*:rate_limit_buckets"
    - from: "lib/agent_com/rate_limiter.ex"
      to: "lib/agent_com/rate_limiter/config.ex"
      via: "Config.defaults/1 for tier thresholds"
      pattern: "Config\\."
---

<objective>
Build the core token bucket rate limiter with ETS-backed lazy refill, action tier classification, progressive backoff, and exemption support.

Purpose: Foundation module that all integration points (WS, HTTP, Scheduler, Admin API, Dashboard) depend on. Using TDD because the token bucket algorithm has well-defined I/O contracts.
Output: Working RateLimiter and RateLimiter.Config modules with comprehensive test coverage, plus ETS tables initialized in Application.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-rate-limiting/15-RESEARCH.md
@.planning/phases/15-rate-limiting/15-CONTEXT.md
@lib/agent_com/validation/violation_tracker.ex (pattern reference -- ETS pure functions)
@lib/agent_com/application.ex (add ETS tables here)
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for RateLimiter and Config</name>
  <files>test/agent_com/rate_limiter_test.exs</files>
  <action>
Create test file with comprehensive tests for the token bucket rate limiter. Tests must cover:

**RateLimiter.Config tests:**
- `ws_tier/1` returns `:light` for "ping", "list_agents", "list_channels", "status", "channel_history"
- `ws_tier/1` returns `:normal` for "message", "channel_publish", "channel_subscribe", "channel_unsubscribe", "task_accepted", "task_progress", "task_complete", "task_failed", "task_recovering"
- `ws_tier/1` returns `:heavy` for "identify"
- `ws_tier/1` returns `:normal` for unknown types (safe default)
- `http_tier/1` returns correct tier for each classified action
- `defaults/1` returns correct capacity and refill_rate for each tier

**RateLimiter core tests:**
- First request initializes bucket and returns `{:allow, remaining}` where remaining = capacity - 1
- Repeated requests decrement tokens; after capacity requests, returns `{:deny, retry_after_ms}`
- `retry_after_ms` is rounded to nearest 1000ms (whole seconds)
- After waiting (simulate time passage), tokens refill via lazy refill
- Warn at 80% usage: when remaining < 20% of capacity, returns `{:warn, remaining}`
- Independent buckets per agent: agent A exhausting tokens does not affect agent B
- Independent buckets per channel: WS and HTTP buckets are separate for same agent
- Independent buckets per tier: exhausting :light does not affect :normal

**Exemption tests:**
- Exempt agent always gets `{:allow, :exempt}` regardless of usage
- Non-exempt agent gets normal rate limiting

**Progressive backoff tests:**
- `record_violation/1` increments consecutive violations
- 1st violation: retry_after_ms = 1000
- 2nd violation: retry_after_ms = 2000
- 3rd violation: retry_after_ms = 5000
- 4th violation: retry_after_ms = 10000
- 5th+ violation: retry_after_ms = 30000
- After quiet period (60s with no violations), consecutive count resets to 0
- `rate_limited?/1` returns true when agent has active violations, false otherwise

**Setup:** Each test creates its own ETS tables (`:rate_limit_buckets`, `:rate_limit_overrides`) in setup block and deletes them in on_exit. Use `async: false` since ETS tables are shared state.

Note on time simulation: For lazy refill tests, you cannot easily mock `System.monotonic_time/1`. Instead, test the refill behavior by:
1. Exhaust tokens to get `{:deny, retry_after_ms}`
2. Use `Process.sleep/1` for a small duration to let real time pass
3. Check that tokens have partially refilled
Keep sleep durations minimal (50-100ms) using tiers with small capacities for test speed. Alternatively, directly insert ETS entries with past timestamps to simulate elapsed time.

Commit: `test(15-01): add failing tests for RateLimiter token bucket and Config tier classification`
  </action>
  <verify>`mix test test/agent_com/rate_limiter_test.exs` -- all tests should FAIL (modules do not exist yet)</verify>
  <done>Test file exists with 15+ test cases covering all token bucket behaviors, tier classification, exemption, and progressive backoff. All tests fail with CompileError or UndefinedFunctionError.</done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement RateLimiter, Config, and ETS initialization</name>
  <files>
    lib/agent_com/rate_limiter.ex
    lib/agent_com/rate_limiter/config.ex
    lib/agent_com/application.ex
  </files>
  <action>
**Create `lib/agent_com/rate_limiter/config.ex`:**

`AgentCom.RateLimiter.Config` module with:
- `ws_tier/1` -- pattern match message type strings to `:light`, `:normal`, `:heavy` tiers using module attributes for the lists (see research for exact classification). Unknown types default to `:normal`.
- `http_tier/1` -- pattern match HTTP action atoms to tiers. Unknown defaults to `:normal`.
- `defaults/1` -- returns `{capacity, refill_rate_per_ms}` tuple for a given tier:
  - `:light` -- capacity 120_000, refill_rate 2.0 (tokens*1000 per ms = 120_000 / 60_000)
  - `:normal` -- capacity 60_000, refill_rate 1.0 (60_000 / 60_000)
  - `:heavy` -- capacity 10_000, refill_rate ~0.1667 (10_000 / 60_000)
  Note: Store refill_rate as float. Capacity in internal units (real_tokens * 1000).

**Create `lib/agent_com/rate_limiter.ex`:**

`AgentCom.RateLimiter` module -- NOT a GenServer. Pure functions + ETS. Follow ViolationTracker pattern.

Module attributes:
- `@bucket_table :rate_limit_buckets`
- `@override_table :rate_limit_overrides`
- `@token_cost 1000` (1 token in internal units)
- `@warn_threshold 0.2` (warn when remaining < 20% of capacity = 80% used)
- `@quiet_period_ms 60_000` (reset consecutive violations after 60s quiet)

Public functions:
- `check(agent_id, channel, tier)` -- Main entry. Returns `{:allow, remaining}`, `{:warn, remaining}`, `{:deny, retry_after_ms}`, or `{:allow, :exempt}`. Uses `exempt?/1` check first. Then lazy refill logic:
  1. Lookup ETS bucket by `{agent_id, channel, tier}` key
  2. Compute elapsed ms since last_refill using `System.monotonic_time(:millisecond)`
  3. Refill tokens: `min(tokens + elapsed * refill_rate, capacity)`
  4. If refilled >= cost: deduct, write back, check warn threshold
  5. If refilled < cost: compute retry_after_ms rounded to nearest second (div + 999 / 1000 * 1000), write back
  6. If no entry: initialize bucket at full capacity minus cost using `get_limits/3`

- `record_violation(agent_id)` -- Increment consecutive violation count in ETS violation entry `{agent_id, :violations}`. Update window_start if new window. Returns the progressive `retry_after_ms` based on consecutive count.

- `rate_limited?(agent_id)` -- Check if agent has active violations (consecutive_violations > 0 and within quiet period). Returns boolean.

- `reset_violations(agent_id)` -- Clear violation entry. For admin use.

- `exempt?(agent_id)` -- Check `:rate_limit_overrides` ETS for `{:whitelist, list}` entry.

- `capacity(agent_id, channel, tier)` -- Return the capacity for display purposes (used in warning frames).

- `get_limits(agent_id, channel, tier)` -- Check for per-agent override in `@override_table` at key `{agent_id, tier}`, fall back to `Config.defaults(tier)`.

- `delete_agent_buckets(agent_id)` -- Delete all bucket entries for an agent (used when overrides change). Use `:ets.match_delete/2` with `{{agent_id, :_, :_}, :_, :_, :_, :_}`.

Progressive backoff lookup (private):
- 1 -> 1000, 2 -> 2000, 3 -> 5000, 4 -> 10000, 5+ -> 30000

Emit telemetry events:
- `[:agent_com, :rate_limit, :check]` with measurements `%{tokens_remaining: remaining}` and metadata `%{agent_id: agent_id, channel: channel, tier: tier, result: :allow | :warn | :deny}`
- `[:agent_com, :rate_limit, :violation]` with measurements `%{retry_after_ms: ms, consecutive: count}` and metadata `%{agent_id: agent_id}`

**Modify `lib/agent_com/application.ex`:**

Add two ETS table creations in `start/2`, after the existing `:validation_backoff` creation:
```elixir
:ets.new(:rate_limit_buckets, [:named_table, :public, :set])
:ets.new(:rate_limit_overrides, [:named_table, :public, :set])
```

Both `:public` so Socket and Plug processes can read/write directly. `:set` for unique composite keys.

Commit: `feat(15-01): implement RateLimiter token bucket with Config tier classification`
  </action>
  <verify>`mix test test/agent_com/rate_limiter_test.exs` -- all tests PASS. Also `mix compile --warnings-as-errors` succeeds.</verify>
  <done>All rate limiter tests pass. RateLimiter.check/3 correctly implements lazy token bucket refill. Config correctly classifies all message types into tiers. Progressive backoff escalates correctly. Exempt agents bypass rate limiting. ETS tables created at application startup.</done>
</task>

</tasks>

<verification>
1. `mix test test/agent_com/rate_limiter_test.exs` -- all tests pass
2. `mix compile --warnings-as-errors` -- no warnings
3. Manual verification: In `iex -S mix`, call `AgentCom.RateLimiter.check("test-agent", :ws, :normal)` and observe `{:allow, 59}` (capacity 60 minus 1 token)
</verification>

<success_criteria>
- RateLimiter module implements lazy token bucket with ETS backend
- Config module classifies all WS and HTTP actions into 3 tiers
- Progressive backoff escalates 1s/2s/5s/10s/30s with 60s quiet reset
- Exempt agents bypass all rate limiting
- Per-agent, per-channel, per-tier bucket isolation
- 15+ tests passing covering all behaviors
- ETS tables created in Application.start
</success_criteria>

<output>
After completion, create `.planning/phases/15-rate-limiting/15-01-SUMMARY.md`
</output>
