---
phase: 19-model-aware-scheduler
plan: 03
type: execute
wave: 3
depends_on: ["19-02"]
files_modified:
  - lib/agent_com/config.ex
  - lib/agent_com/alerter.ex
  - lib/agent_com/scheduler.ex
autonomous: true

must_haves:
  truths:
    - "Fallback wait timeout is configurable at runtime via Config (default 5s)"
    - "Task TTL for tier-unavailable tasks is configurable (default 10min)"
    - "Tier-down alert fires when a tier stays down beyond configurable threshold (default 60s)"
    - "Queued tasks with expired TTL are swept and expired rather than building unbounded backlog"
    - "Trivial tasks continue executing locally when all non-trivial tiers are down"
  artifacts:
    - path: "lib/agent_com/config.ex"
      provides: "Default config values for routing timeouts"
      contains: "fallback_wait_ms"
    - path: "lib/agent_com/alerter.ex"
      provides: "Tier-down alert rule"
      contains: "tier_down"
    - path: "lib/agent_com/scheduler.ex"
      provides: "TTL sweep for queued tasks, configurable fallback timeout"
      contains: "task_ttl"
  key_links:
    - from: "lib/agent_com/scheduler.ex"
      to: "lib/agent_com/config.ex"
      via: "Config.get for routing timeouts"
      pattern: "Config\\.get\\(:fallback_wait_ms\\)"
    - from: "lib/agent_com/alerter.ex"
      to: "lib/agent_com/llm_registry.ex"
      via: "LlmRegistry.list_endpoints for tier health check"
      pattern: "LlmRegistry\\.list_endpoints"
---

<objective>
Add runtime configuration for routing timeouts, tier-down alert rules, and task TTL sweep for degraded behavior handling.

Purpose: Implement the locked decisions for degraded behavior -- TTL-based task expiry prevents unbounded backlog, tier-down alerts notify operators after configurable duration, and all routing timeouts are adjustable without restart.

Output: Configurable routing parameters, tier-down alerter rule, task TTL sweep in scheduler.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/19-model-aware-scheduler/19-CONTEXT.md
@.planning/phases/19-model-aware-scheduler/19-RESEARCH.md
@.planning/phases/19-model-aware-scheduler/19-02-SUMMARY.md
@lib/agent_com/config.ex
@lib/agent_com/alerter.ex
@lib/agent_com/scheduler.ex
@lib/agent_com/llm_registry.ex
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add routing config defaults and tier-down alert rule</name>
  <files>
    lib/agent_com/config.ex
    lib/agent_com/alerter.ex
  </files>
  <action>
    **Config (lib/agent_com/config.ex):**

    Add new entries to `@defaults` map:
    ```elixir
    @defaults %{
      heartbeat_interval_ms: 900_000,
      # Routing timeouts (Phase 19)
      fallback_wait_ms: 5_000,
      task_ttl_ms: 600_000,           # 10 minutes
      tier_down_alert_threshold_ms: 60_000,  # 60 seconds
      default_ollama_model: "qwen2.5-coder:7b"
    }
    ```

    These are read via `Config.get(:fallback_wait_ms)` etc. and can be updated at runtime via `Config.put/2` without restart.

    **Alerter (lib/agent_com/alerter.ex):**

    Add a 6th alert rule: `tier_down` (WARNING severity).

    In the `evaluate_rules/1` function (or wherever rules are evaluated), add:

    Rule: **tier_down** (WARNING)
    - Check `AgentCom.LlmRegistry.list_endpoints()` for all endpoints unhealthy or empty
    - Track when the "all unhealthy" condition was first observed (store timestamp in alerter state or use a simple approach)
    - Only fire alert when the condition persists beyond `Config.get(:tier_down_alert_threshold_ms)` (default 60s)
    - This matches the locked decision: "alert after threshold -- only alert when a tier stays down beyond a configurable duration, not on brief blips"
    - Alert message: "Ollama tier down: no healthy endpoints for #{duration}s"
    - Details: `%{unhealthy_endpoints: [ids], duration_ms: ms}`

    Implementation approach: Follow the existing Alerter pattern. The Alerter already evaluates rules periodically against metrics. Add:
    1. A new rule ID `:tier_down` in the rules list
    2. In the evaluation function, call `LlmRegistry.list_endpoints()` and check if any are `:healthy`
    3. Use a `:tier_down_since` field in alerter state (set to timestamp when first all-unhealthy, cleared when any recover)
    4. Only fire when `now - tier_down_since > threshold`

    IMPORTANT: The Alerter already has a well-established pattern for rules. Read the current alerter code and follow its exact pattern for adding a new rule. The existing rules use `MetricsCollector.snapshot/0` for data -- this rule adds a new data source (LlmRegistry) which is acceptable since the alerter already queries fresh data on each cycle.
  </action>
  <verify>
    `mix compile --warnings-as-errors` -- no warnings.
    `mix test test/agent_com/alerter_test.exs` -- existing tests pass, new test for tier_down if alerter tests exist.
    `Config.get(:fallback_wait_ms)` returns 5000 (verify via `mix run -e "IO.inspect AgentCom.Config.get(:fallback_wait_ms)"` or in test).
  </verify>
  <done>
    Config defaults include fallback_wait_ms (5s), task_ttl_ms (10min), tier_down_alert_threshold_ms (60s), default_ollama_model. Alerter evaluates tier_down rule based on LlmRegistry endpoint health with configurable threshold duration.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add task TTL sweep and configurable fallback timeout in Scheduler</name>
  <files>lib/agent_com/scheduler.ex</files>
  <action>
    **Scheduler (lib/agent_com/scheduler.ex):**

    1. **Configurable fallback timeout:** Replace the hardcoded 5000ms fallback timer with `AgentCom.Config.get(:fallback_wait_ms)` read at the point of setting the timer. This way the timeout is configurable at runtime.

    2. **Task TTL sweep:** Add a periodic sweep that expires queued tasks older than TTL.

    Add a new timer in init:
    ```elixir
    Process.send_after(self(), :sweep_ttl, 60_000)  # Check every 60s
    ```

    Add handler:
    ```elixir
    def handle_info(:sweep_ttl, state) do
      ttl_ms = AgentCom.Config.get(:task_ttl_ms) || 600_000
      now = System.system_time(:millisecond)
      cutoff = now - ttl_ms

      queued_tasks = AgentCom.TaskQueue.list(status: :queued)

      expired_count =
        queued_tasks
        |> Enum.filter(fn task ->
          # Only expire tasks that have been waiting AND have a pending fallback that already fired
          # or have been queued longer than TTL without assignment
          task.created_at < cutoff
        end)
        |> Enum.each(fn task ->
          Logger.warning("scheduler_task_ttl_expired",
            task_id: task.id,
            age_ms: now - task.created_at,
            ttl_ms: ttl_ms
          )

          # Expire by failing the task with a TTL expiry error
          # Use fail_task with generation 0 won't work (task is queued, not assigned).
          # Instead, move directly to dead-letter via a new TTL-specific path.
          # Simplest approach: complete the task with a TTL expiry result.
          # Actually, the cleanest approach for a queued task: use the existing dead-letter
          # mechanism but we need to handle queued->dead_letter transition.
          #
          # PRAGMATIC APPROACH: Leave task queued but mark it with an expiry note.
          # For Phase 19, log the expiry and let the existing dead-letter mechanism
          # handle it when it eventually gets assigned and fails. OR:
          # Remove from queue and broadcast a task_expired event.
          expire_task(task.id)
        end)

      Process.send_after(self(), :sweep_ttl, 60_000)
      {:noreply, state}
    end
    ```

    Add `expire_task/1` that calls into TaskQueue. Add a new TaskQueue function `expire_task/1` if needed, or reuse existing mechanisms. The simplest path: add a TaskQueue.expire_task/1 that moves a queued task directly to dead_letter with reason "ttl_expired". This is a small addition:

    In TaskQueue, add:
    ```elixir
    def expire_task(task_id) do
      GenServer.call(__MODULE__, {:expire_task, task_id})
    end
    ```

    Handler: look up task, verify status :queued, move to dead_letter table with status :dead_letter and last_error "ttl_expired", remove from priority index, broadcast :task_dead_letter event.

    3. **Trivial tasks bypass:** Per locked decision "trivial tasks still execute locally" when all tiers are down. The TaskRouter already routes trivial tasks to :sidecar regardless of endpoint availability. The TTL sweep should NOT expire trivial-tier tasks. Add a check:
    ```elixir
    |> Enum.filter(fn task ->
      tier = get_in(task, [:complexity, :effective_tier])
      task.created_at < cutoff and tier != :trivial
    end)
    ```

    This ensures trivial tasks are never expired by TTL -- they stay in queue and route to sidecar when an agent becomes available.
  </action>
  <verify>
    `mix compile --warnings-as-errors` -- no warnings.
    `mix test` -- full test suite passes, no regressions.
  </verify>
  <done>
    Scheduler reads fallback_wait_ms from Config (runtime configurable). TTL sweep expires non-trivial queued tasks older than task_ttl_ms. Trivial tasks are exempt from TTL expiry per locked decision. TaskQueue.expire_task/1 moves queued tasks to dead_letter.
  </done>
</task>

</tasks>

<verification>
1. `mix compile --warnings-as-errors` -- no warnings
2. `mix test` -- full test suite passes
3. Config defaults are accessible: fallback_wait_ms, task_ttl_ms, tier_down_alert_threshold_ms, default_ollama_model
4. Alerter has 6 rules (5 existing + tier_down)
</verification>

<success_criteria>
- All routing timeouts are configurable via Config.get/put without restart
- Tier-down alert fires only after threshold duration (not on brief blips)
- TTL sweep expires non-trivial queued tasks, prevents unbounded backlog
- Trivial tasks are never expired (they execute locally regardless of tier availability)
</success_criteria>

<output>
After completion, create `.planning/phases/19-model-aware-scheduler/19-03-SUMMARY.md`
</output>
