---
phase: 19-model-aware-scheduler
plan: 02
type: execute
wave: 2
depends_on: ["19-01"]
files_modified:
  - lib/agent_com/scheduler.ex
  - lib/agent_com/task_queue.ex
  - lib/agent_com/telemetry.ex
  - test/agent_com/scheduler_test.exs
autonomous: true

must_haves:
  truths:
    - "Scheduler queries LlmRegistry endpoints once per scheduling round and passes them to TaskRouter"
    - "Routing decision is stored on the task map when assigned"
    - "Fallback timer fires after 5s timeout and retries at the next tier"
    - "Pending fallback timers are cancelled on task assignment, completion, reclaim, and dead-letter"
    - "Scheduler subscribes to llm_registry PubSub and re-evaluates on endpoint recovery"
    - "Routing telemetry event is emitted for every routing decision with full metadata"
    - "Trivial tasks are routed to sidecar (no agent assignment needed -- task_data includes routing_decision)"
  artifacts:
    - path: "lib/agent_com/scheduler.ex"
      provides: "Augmented scheduler with tier-aware routing, fallback timers, llm_registry subscription"
      contains: "TaskRouter.route"
    - path: "lib/agent_com/task_queue.ex"
      provides: "routing_decision field on task map"
      contains: "routing_decision"
    - path: "lib/agent_com/telemetry.ex"
      provides: "New scheduler:route telemetry event definition"
      contains: "scheduler, :route"
    - path: "test/agent_com/scheduler_test.exs"
      provides: "Updated scheduler tests covering routing integration"
  key_links:
    - from: "lib/agent_com/scheduler.ex"
      to: "lib/agent_com/task_router.ex"
      via: "TaskRouter.route/3 in try_schedule_all"
      pattern: "TaskRouter\\.route"
    - from: "lib/agent_com/scheduler.ex"
      to: "lib/agent_com/llm_registry.ex"
      via: "LlmRegistry.list_endpoints/0 in try_schedule_all"
      pattern: "LlmRegistry\\.list_endpoints"
    - from: "lib/agent_com/scheduler.ex"
      to: "lib/agent_com/llm_registry.ex"
      via: "LlmRegistry.get_resources/1 for endpoint scoring"
      pattern: "LlmRegistry\\.get_resources"
---

<objective>
Wire the TaskRouter into the Scheduler GenServer, adding fallback timer state management, llm_registry PubSub subscription, routing decision storage on tasks, and routing telemetry.

Purpose: Transform the scheduler from simple capability matching to tier-aware routing that picks the best endpoint for each task based on complexity, model availability, and host load.

Output: Augmented Scheduler, TaskQueue with routing_decision field, routing telemetry event.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/19-model-aware-scheduler/19-CONTEXT.md
@.planning/phases/19-model-aware-scheduler/19-RESEARCH.md
@.planning/phases/19-model-aware-scheduler/19-01-SUMMARY.md
@lib/agent_com/scheduler.ex
@lib/agent_com/task_queue.ex
@lib/agent_com/llm_registry.ex
@lib/agent_com/telemetry.ex
@test/agent_com/scheduler_test.exs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add routing_decision field to TaskQueue and routing telemetry event</name>
  <files>
    lib/agent_com/task_queue.ex
    lib/agent_com/telemetry.ex
  </files>
  <action>
    **TaskQueue (lib/agent_com/task_queue.ex):**

    In `handle_call({:submit, params}, ...)`, add `routing_decision: nil` to the task map, after the `complexity` field. This field will be populated by the Scheduler when it routes the task.

    Add a new public function `store_routing_decision/2` that updates the routing_decision field on a task:

    ```elixir
    @doc "Store routing decision on a task. Called by Scheduler after routing."
    def store_routing_decision(task_id, routing_decision) do
      GenServer.call(__MODULE__, {:store_routing_decision, task_id, routing_decision})
    end
    ```

    Implement the handler:
    - Look up the task
    - Update routing_decision field
    - Persist to DETS
    - Return {:ok, task} or {:error, :not_found}

    **Telemetry (lib/agent_com/telemetry.ex):**

    Add new event to the catalog documentation:

    ```
    ### Scheduler Routing

    - `[:agent_com, :scheduler, :route]` - Task routing decision made
      measurements: `%{candidate_count: integer, scoring_duration_us: integer}`
      metadata: `%{task_id, effective_tier, target_type, selected_endpoint, selected_model,
                    fallback_used, fallback_reason, classification_reason, estimated_cost_tier}`
    ```

    Add `[:agent_com, :scheduler, :route]` to the events list in `attach_handlers/0`.

    Also add `[:agent_com, :scheduler, :fallback]` event for when a fallback timer fires:
    measurements: `%{wait_ms: integer}`
    metadata: `%{task_id, original_tier, fallback_tier}`
  </action>
  <verify>
    `mix compile --warnings-as-errors` passes. TaskQueue tests still pass: `mix test test/agent_com/task_queue_test.exs`.
  </verify>
  <done>
    TaskQueue task map includes routing_decision: nil. store_routing_decision/2 function exists. Telemetry module registers scheduler:route and scheduler:fallback events.
  </done>
</task>

<task type="auto">
  <name>Task 2: Augment Scheduler with tier routing, fallback timers, and llm_registry subscription</name>
  <files>
    lib/agent_com/scheduler.ex
    test/agent_com/scheduler_test.exs
  </files>
  <action>
    **Scheduler (lib/agent_com/scheduler.ex):**

    1. **State:** Change init state from `%{}` to `%{pending_fallbacks: %{}}`. The pending_fallbacks map is keyed by task_id with value `%{original_tier: atom, fallback_tier: atom, timer_ref: ref, queued_at: integer}`.

    2. **PubSub subscription:** In init, add `Phoenix.PubSub.subscribe(AgentCom.PubSub, "llm_registry")`. Add handler:
    ```elixir
    def handle_info({:llm_registry_update, :endpoint_changed}, state) do
      try_schedule_all(:endpoint_changed)
      {:noreply, state}
    end
    ```

    3. **Augment try_schedule_all:** After getting idle_agents and queued_tasks, also fetch endpoints and resources:
    ```elixir
    endpoints = AgentCom.LlmRegistry.list_endpoints()
    endpoint_resources = gather_endpoint_resources(endpoints)
    ```
    Pass these to `do_match_loop/4` (was `do_match_loop/2`).

    `gather_endpoint_resources/1` iterates endpoints, calls `LlmRegistry.get_resources(ep.id)` for each, returns a map of `%{endpoint_id => resources}`.

    4. **Augment do_match_loop:** Before the existing agent matching, call `TaskRouter.route(task, endpoints, endpoint_resources)` for each task:

    - `{:ok, decision}` where `decision.target_type == :sidecar` -> Handle trivial routing. For now, log the routing decision and store it on the task. The actual sidecar direct execution is Phase 20. For Phase 19: assign trivial tasks to any idle agent (existing behavior) but include the routing_decision so the sidecar knows to execute locally. Emit routing telemetry.

    - `{:ok, decision}` where `decision.target_type == :ollama` -> Find an idle agent whose `ollama_url` host matches `decision.selected_endpoint` (or any idle agent if no match). Assign task with routing_decision. Emit routing telemetry.

    - `{:ok, decision}` where `decision.target_type == :claude` -> Assign to any idle agent (Phase 20 handles the actual Claude call). Store routing_decision. Emit routing telemetry.

    - `{:fallback, tier, reason}` -> Check one-step fallback. Call `TierResolver.fallback_up(tier)`. If fallback tier exists, set a Process.send_after timer for 5000ms with `{:fallback_timeout, task_id}`. Store in pending_fallbacks. Skip this task for now (continue to next task in loop). If no fallback tier available, skip task (it stays queued).

    5. **Fallback timer handler:**
    ```elixir
    def handle_info({:fallback_timeout, task_id}, state) do
      case Map.pop(state.pending_fallbacks, task_id) do
        {nil, _} -> {:noreply, state}  # already handled
        {fallback_info, remaining} ->
          # Re-check if task is still queued
          case AgentCom.TaskQueue.get(task_id) do
            {:ok, %{status: :queued} = task} ->
              # Attempt routing at fallback tier -- use TaskRouter with overridden tier
              # For simplicity: trigger a full scheduling attempt which will re-evaluate
              try_schedule_all(:fallback_timeout)
              {:noreply, %{state | pending_fallbacks: remaining}}
            _ ->
              {:noreply, %{state | pending_fallbacks: remaining}}
          end
      end
    end
    ```

    Emit `[:agent_com, :scheduler, :fallback]` telemetry when fallback fires.

    6. **Fallback cleanup helper:** Add `cancel_pending_fallback/2`:
    ```elixir
    defp cancel_pending_fallback(state, task_id) do
      case Map.pop(state.pending_fallbacks, task_id) do
        {nil, _} -> state
        {info, remaining} ->
          Process.cancel_timer(info.timer_ref)
          %{state | pending_fallbacks: remaining}
      end
    end
    ```

    Call this from: do_assign (after successful assignment), and add cleanup to task_completed, task_reclaimed, task_dead_letter event handlers. Update those handlers from just calling try_schedule_all to also cleaning up fallbacks:
    ```elixir
    def handle_info({:task_event, %{event: :task_completed, task_id: tid}}, state) do
      state = cancel_pending_fallback(state, tid)
      try_schedule_all(:task_completed)
      {:noreply, state}
    end
    ```
    Similarly for task_reclaimed. Add a handler for task_dead_letter that cleans up fallback (no scheduling needed).

    7. **do_assign augmentation:** Change `do_assign/2` to `do_assign/3` accepting routing_decision. Store routing decision on the task via `TaskQueue.store_routing_decision(task.id, routing_decision)`. Include routing_decision in task_data sent to the agent via Socket.

    **Scheduler Tests (test/agent_com/scheduler_test.exs):**

    Add new test(s):
    - "routing decision is stored on assigned task" -- submit a task, verify it gets assigned, check that `TaskQueue.get(task.id)` has a non-nil routing_decision field.
    - Existing tests should continue to pass since the routing logic is additive (when no endpoints are registered, TaskRouter returns fallback, but the scheduler still assigns using existing capability matching as fallback behavior).

    IMPORTANT: For backward compatibility when no LLM endpoints are registered (which is the case in existing tests), the scheduler should fall back gracefully. If TaskRouter returns `{:fallback, :standard, reason}` and there's no fallback tier with capacity, the scheduler should still attempt the existing capability-matching logic. This ensures existing tests pass without modification. The routing_decision will indicate `fallback_used: true` with reason.
  </action>
  <verify>
    `mix test test/agent_com/scheduler_test.exs` -- all existing tests pass plus new routing test.
    `mix compile --warnings-as-errors` -- no warnings.
  </verify>
  <done>
    Scheduler queries LlmRegistry and calls TaskRouter.route/3 for every queued task. Routing decisions are stored on tasks. Fallback timers work with proper cleanup. llm_registry PubSub triggers re-evaluation. Routing telemetry emitted for every decision. Existing scheduler tests still pass.
  </done>
</task>

</tasks>

<verification>
1. `mix test test/agent_com/scheduler_test.exs` -- all tests pass
2. `mix test test/agent_com/task_queue_test.exs` -- all tests pass
3. `mix compile --warnings-as-errors` -- no warnings
4. Full test suite: `mix test` -- no regressions
</verification>

<success_criteria>
- Scheduler routes every task through TaskRouter before assignment
- Routing decisions are persisted on task maps in DETS
- Fallback timers fire correctly and clean up on all task state changes
- llm_registry PubSub subscription triggers scheduling attempts on endpoint recovery
- All existing scheduler tests pass without modification (backward compatible)
</success_criteria>

<output>
After completion, create `.planning/phases/19-model-aware-scheduler/19-02-SUMMARY.md`
</output>
