---
phase: 38-ollama-client-hub-routing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/agent_com/ollama_client.ex
  - test/agent_com/ollama_client_test.exs
  - config/config.exs
  - config/test.exs
autonomous: true

must_haves:
  truths:
    - "OllamaClient.chat/2 sends a POST to Ollama /api/chat and returns parsed content + token counts"
    - "OllamaClient handles connection errors, HTTP errors, and JSON decode errors with tagged tuples"
    - "Config controls Ollama host, port, model, and timeout with sensible defaults"
  artifacts:
    - path: "lib/agent_com/ollama_client.ex"
      provides: "Stateless HTTP wrapper for Ollama /api/chat"
      exports: ["chat/2"]
    - path: "test/agent_com/ollama_client_test.exs"
      provides: "Unit tests for request building, response parsing, error handling"
    - path: "config/config.exs"
      provides: "Ollama config keys with defaults"
  key_links:
    - from: "lib/agent_com/ollama_client.ex"
      to: "Ollama /api/chat HTTP endpoint"
      via: ":httpc.request/4 POST"
      pattern: "httpc\\.request.*:post"
    - from: "lib/agent_com/ollama_client.ex"
      to: "Jason"
      via: "JSON encode/decode"
      pattern: "Jason\\.(encode|decode)"
---

<objective>
Build the OllamaClient HTTP module that wraps Ollama's /api/chat endpoint.

Purpose: Provide the hub with a direct HTTP path to local Ollama models, replacing the System.cmd("claude", ["-p", ...]) approach. This is the foundation that Plan 38-02 will route through.
Output: `lib/agent_com/ollama_client.ex` -- stateless module with `chat/2` function, plus unit tests and config.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/38-ollama-client-hub-routing/38-RESEARCH.md
@.planning/phases/38-ollama-client-hub-routing/38-CONTEXT.md
@lib/agent_com/llm_registry.ex
@lib/agent_com/claude_client.ex
@lib/agent_com/claude_client/cli.ex
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OllamaClient module with chat/2 and config</name>
  <files>lib/agent_com/ollama_client.ex, config/config.exs, config/test.exs</files>
  <action>
Create `lib/agent_com/ollama_client.ex` as a stateless module (NOT a GenServer -- ClaudeClient GenServer handles serialization). Follow the `:httpc` patterns from `lib/agent_com/llm_registry.ex` lines 316-340.

Module: `AgentCom.OllamaClient`

Public API:
- `chat(prompt, opts \\ [])` -- sends a chat request to Ollama /api/chat
  - `prompt` is a string (the user message content)
  - `opts` keyword list: `:system` (system message string), `:model`, `:host`, `:port`, `:timeout`, `:tools` (list of tool defs, nil by default)
  - Returns `{:ok, %{content: string, prompt_tokens: integer, eval_tokens: integer, total_duration_ns: integer}}` or `{:error, term()}`

Implementation details:
1. `build_messages(system, prompt)` -- builds messages list with optional system message + user message
2. `build_body(model, messages, tools)` -- builds request map with `"stream" => false`, `"options" => %{"temperature" => 0.3, "num_ctx" => 8192}`; include `"tools"` key only if tools is not nil/empty
3. `do_post(url, body, timeout)` -- POST via `:httpc.request(:post, {url, headers, content_type, body}, http_opts, [])` where:
   - `url` is a charlist: `String.to_charlist("http://#{host}:#{port}/api/chat")`
   - `headers` is `[{~c"content-type", ~c"application/json"}]`
   - `content_type` is `~c"application/json"`
   - `body` is `Jason.encode!(request_map)`
   - `http_opts` is `[timeout: timeout, connect_timeout: 5_000]`
   - Handle: `{:ok, {{_, 200, _}, _, resp_body}}` -> decode JSON; `{:ok, {{_, status, _}, _, resp_body}}` -> `{:error, {:http_error, status, to_string(resp_body)}}`; `{:error, reason}` -> `{:error, {:connection_error, reason}}`
   - Wrap in try/rescue for unexpected errors
4. `parse_response(response_map)` -- extracts from Ollama response:
   - `content` from `response["message"]["content"]`
   - `prompt_tokens` from `response["prompt_eval_count"]` (default 0)
   - `eval_tokens` from `response["eval_count"]` (default 0)
   - `total_duration_ns` from `response["total_duration"]` (default 0)
   - Strip `<think>...</think>` blocks from content (Qwen3 thinking mode)
   - Return `{:ok, %{content: content, prompt_tokens: ..., eval_tokens: ..., total_duration_ns: ...}}`
   - Handle missing "message" key: `{:error, {:unexpected_format, response_map}}`

5. `strip_thinking(content)` -- removes `<think>...</think>` blocks using `Regex.replace(~r/<think>.*?<\/think>/s, content, "")` then `String.trim/1`

Config helper: `defp config(key, default), do: Application.get_env(:agent_com, key, default)`

Default config values (used as fallbacks in module):
- `:ollama_host` -> `"localhost"`
- `:ollama_port` -> `11434`
- `:ollama_model` -> `"qwen3:8b"`
- `:ollama_timeout_ms` -> `120_000`

Add to `config/config.exs` BEFORE the `import_config` line:
```elixir
# Ollama LLM backend (Phase 38)
config :agent_com,
  llm_backend: :ollama,
  ollama_host: "localhost",
  ollama_port: 11434,
  ollama_model: "qwen3:8b",
  ollama_timeout_ms: 120_000
```

Add to `config/test.exs`:
```elixir
# Use claude_cli backend in test (OllamaClient tested directly with mocks)
config :agent_com, llm_backend: :claude_cli
```

Add `require Logger` and log at info level on chat calls (model, prompt byte size) and on responses (status, duration).
  </action>
  <verify>
`mix compile --warnings-as-errors` passes cleanly. `grep -r "OllamaClient" lib/agent_com/ollama_client.ex` shows the module exists. `grep "llm_backend" config/config.exs` shows config key.
  </verify>
  <done>
OllamaClient module compiles with chat/2 public API, config keys exist with defaults, test config overrides backend to :claude_cli.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for OllamaClient</name>
  <files>test/agent_com/ollama_client_test.exs</files>
  <action>
Create `test/agent_com/ollama_client_test.exs` testing the pure functions in OllamaClient. Since `:httpc` calls are hard to mock without a dependency, test the internal parsing and building logic by making key private functions testable via `@doc false` public helpers or by testing through the public API with a bypass approach.

Strategy: Test the functions that DON'T require HTTP by extracting them as `@doc false` public functions if needed, or test through chat/2 with Ollama not running (expecting connection error).

Tests:
1. **`test "parse_response/1 extracts content and token counts"`** -- Call the parse function directly with a valid Ollama response map: `%{"message" => %{"role" => "assistant", "content" => "hello"}, "prompt_eval_count" => 10, "eval_count" => 20, "total_duration" => 5000000}`. Assert content is "hello", prompt_tokens is 10, eval_tokens is 20.

2. **`test "parse_response/1 strips thinking blocks"`** -- Response with content `"<think>reasoning here</think>\nactual answer"`. Assert content is `"actual answer"` (thinking block stripped).

3. **`test "parse_response/1 handles missing token counts"`** -- Response with only `"message"` key, no eval counts. Assert defaults to 0.

4. **`test "parse_response/1 returns error for unexpected format"`** -- Pass `%{"error" => "model not found"}`. Assert `{:error, {:unexpected_format, _}}`.

5. **`test "chat/2 returns connection error when Ollama not running"`** -- Call `chat("hello", host: "localhost", port: 19999, timeout: 1000)`. Assert `{:error, {:connection_error, _}}`.

6. **`test "build_messages/2 creates system + user messages"`** -- Verify message list format.

7. **`test "build_body/3 includes tools only when provided"`** -- Verify tools key absent when nil, present when given.

Mark the parse/build functions as `@doc false` public if they are currently private, to enable direct testing. This is the pattern used in `ClaudeClient.Response` (the `extract_xml_block` is `@doc false`).

Use `ExUnit.Case, async: true` since these are pure function tests with no GenServer interaction.
  </action>
  <verify>
`mix test test/agent_com/ollama_client_test.exs` -- all tests pass. At minimum, parse and build tests pass (connection error test may need Ollama not running on port 19999, which is typical in CI/test).
  </verify>
  <done>
7 unit tests covering response parsing (with thinking block stripping), message building, body building, and error handling. All pass.
  </done>
</task>

</tasks>

<verification>
1. `mix compile --warnings-as-errors` passes
2. `mix test test/agent_com/ollama_client_test.exs` passes
3. `lib/agent_com/ollama_client.ex` exists with `chat/2` function
4. `config/config.exs` has `llm_backend: :ollama` and Ollama config keys
5. `config/test.exs` has `llm_backend: :claude_cli`
</verification>

<success_criteria>
- OllamaClient.chat/2 compiles and has correct typespec
- Response parsing correctly extracts content, token counts, and strips thinking blocks
- Config keys established with sensible defaults
- Unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/38-ollama-client-hub-routing/38-01-SUMMARY.md`
</output>
