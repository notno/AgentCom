---
phase: 39-pipeline-reliability
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - sidecar/index.js
  - sidecar/lib/execution/verification-loop.js
  - lib/agent_com/scheduler.ex
  - lib/agent_com/task_queue.ex
  - lib/agent_com/socket.ex
autonomous: true
must_haves:
  truths:
    - "Agentic task exceeding 30-minute deadline is killed and reported as timeout failure"
    - "Task stuck in working state for >10 minutes with offline agent is automatically requeued"
    - "Hub handles wake_result messages and logs them"
    - "Generation counter is checked by sidecar before executing and by hub before accepting results"
  artifacts:
    - path: "sidecar/index.js"
      provides: "Execution timeout wrapper in executeTask(), generation check before execute"
    - path: "sidecar/lib/execution/verification-loop.js"
      provides: "Timeout-aware executeWithVerification"
    - path: "lib/agent_com/scheduler.ex"
      provides: "Agent-aware stuck detection with configurable threshold"
    - path: "lib/agent_com/socket.ex"
      provides: "wake_result message handler"
  key_links:
    - from: "sidecar/index.js executeTask()"
      to: "sidecar/lib/execution/verification-loop.js"
      via: "Promise.race with timeout"
      pattern: "Promise\\.race"
    - from: "lib/agent_com/scheduler.ex sweep_stuck"
      to: "lib/agent_com/agent_fsm.ex"
      via: "get_state check before reclaim"
      pattern: "AgentFSM\\.get_state"
---

<objective>
Add execution timeouts, agent-aware stuck detection, hub wake_result handling, and generation-based idempotency.

Purpose: PIPE-02 prevents runaway tasks, PIPE-03 recovers from stuck tasks with offline agents, PIPE-04 prevents ghost results from stale assignments. The hub also needs to handle wake_result messages from Plan 39-01.

Output: Timeout-protected execution, smart stuck detection, generation-fenced execution, wake_result hub handling.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/39-pipeline-reliability/39-RESEARCH.md
@sidecar/index.js
@sidecar/lib/execution/verification-loop.js
@lib/agent_com/scheduler.ex
@lib/agent_com/task_queue.ex
@lib/agent_com/socket.ex
@lib/agent_com/agent_fsm.ex
</context>

<tasks>

<task type="auto">
  <name>Task 1: PIPE-02 -- Execution timeout and PIPE-04 -- Generation check in sidecar</name>
  <files>sidecar/index.js</files>
  <action>
**PIPE-02: Add execution timeout to executeTask().**

Wrap the `executeWithVerification` call in `executeTask()` (line 789) with a `Promise.race` against a timeout. The timeout should be based on task complexity tier:

```javascript
async executeTask(task) {
    // ... existing workspace resolution code ...

    // PIPE-02: Determine execution timeout based on complexity tier
    const complexityTier = task.complexity?.effective_tier || 'standard';
    const TIMEOUT_MS = {
        'trivial': 600000,    // 10 minutes
        'standard': 1800000,  // 30 minutes
        'complex': 1800000    // 30 minutes
    }[complexityTier] || 1800000;

    let timeoutHandle;
    const timeoutPromise = new Promise((_, reject) => {
        timeoutHandle = setTimeout(() => {
            reject(new Error('execution_timeout'));
        }, TIMEOUT_MS);
    });

    // ... existing emitter setup ...

    try {
        task.status = 'working';
        saveQueue(QUEUE_PATH, _queue);

        const result = await Promise.race([
            executeWithVerification(task, effectiveConfig, (event) => emitter.emit(event)),
            timeoutPromise
        ]);

        clearTimeout(timeoutHandle);
        emitter.flush();
        emitter.destroy();

        this.sendTaskComplete(task.task_id, { /* ... existing fields ... */ });
    } catch (err) {
        clearTimeout(timeoutHandle);
        emitter.destroy();

        if (err.message === 'execution_timeout') {
            log('error', 'execution_timeout', {
                task_id: task.task_id,
                timeout_ms: TIMEOUT_MS,
                complexity_tier: complexityTier
            });
            this.sendTaskFailed(task.task_id, `execution_timeout_${TIMEOUT_MS}ms`);
        } else {
            log('error', 'execution_failed', { task_id: task.task_id, error: err.message });
            this.sendTaskFailed(task.task_id, err.message);
        }
    }

    _queue.active = null;
    saveQueue(QUEUE_PATH, _queue);
}
```

Important: Always call `clearTimeout(timeoutHandle)` on both success and failure paths to prevent timer leaks.

**PIPE-04: Add generation check before executing.**

In `handleTaskAssign()`, the generation is already tracked via `this.taskGenerations.set(msg.task_id, msg.generation)` (line 664). Add a generation check at the start of `executeTask()` to skip stale assignments:

```javascript
async executeTask(task) {
    // PIPE-04: Check generation is still current
    const expectedGen = this.taskGenerations.get(task.task_id);
    if (expectedGen !== undefined && task.generation !== expectedGen) {
        log('warning', 'stale_generation_skip', {
            task_id: task.task_id,
            task_generation: task.generation,
            expected_generation: expectedGen
        });
        _queue.active = null;
        saveQueue(QUEUE_PATH, _queue);
        return;
    }
    // ... rest of executeTask ...
}
```
  </action>
  <verify>
1. Search for `Promise.race` in index.js -- should find it wrapping executeWithVerification.
2. Search for `clearTimeout(timeoutHandle)` -- should find it in both success and catch paths.
3. Search for `execution_timeout` -- should find the error message and the log event.
4. Search for `stale_generation_skip` -- should find the generation check at the start of executeTask.
5. Verify the timeout values: trivial=600000, standard=1800000, complex=1800000.
  </verify>
  <done>executeTask() wraps execution in a Promise.race timeout (10min trivial, 30min standard/complex), reports execution_timeout on deadline, clears timers on all paths, and checks generation before executing.</done>
</task>

<task type="auto">
  <name>Task 2: PIPE-03 -- Agent-aware stuck detection and hub wake_result handler</name>
  <files>lib/agent_com/scheduler.ex, lib/agent_com/socket.ex</files>
  <action>
**PIPE-03: Enhance stuck sweep in scheduler.ex.**

Modify the `:sweep_stuck` handler (lines 193-215) to:
1. Change `@stuck_threshold_ms` from 300_000 (5 min) to 600_000 (10 min).
2. Before reclaiming, check if the agent is still online via AgentFSM.get_state().
3. Only reclaim if the agent is offline (FSM not found) or the task is stale beyond the threshold.

```elixir
@stuck_threshold_ms 600_000  # Changed from 300_000 to 600_000 (10 minutes)

def handle_info(:sweep_stuck, state) do
    now = System.system_time(:millisecond)
    threshold = now - @stuck_threshold_ms

    assigned_tasks = AgentCom.TaskQueue.list(status: :assigned)

    Enum.each(assigned_tasks, fn task ->
      if task.updated_at < threshold do
        staleness_ms = now - task.updated_at
        staleness_min = Float.round(staleness_ms / 60_000, 1)

        # PIPE-03: Check agent online status before reclaiming
        agent_status = case AgentCom.AgentFSM.get_state(task.assigned_to) do
          {:ok, %{fsm_state: fsm_state}} -> fsm_state
          {:error, :not_found} -> :offline
        end

        case agent_status do
          :offline ->
            Logger.warning("scheduler_reclaim_stuck_offline",
              task_id: task.id,
              assigned_to: task.assigned_to,
              stale_minutes: staleness_min,
              agent_status: :offline
            )
            AgentCom.TaskQueue.reclaim_task(task.id)

          :working ->
            # Agent is online and working -- only reclaim if VERY stale (2x threshold)
            if task.updated_at < (now - @stuck_threshold_ms * 2) do
              Logger.warning("scheduler_reclaim_stuck_unresponsive",
                task_id: task.id,
                assigned_to: task.assigned_to,
                stale_minutes: staleness_min,
                agent_status: :working
              )
              AgentCom.TaskQueue.reclaim_task(task.id)
            end

          _other ->
            # Agent in unexpected state (idle, assigned, blocked) -- reclaim
            Logger.warning("scheduler_reclaim_stuck_unexpected",
              task_id: task.id,
              assigned_to: task.assigned_to,
              stale_minutes: staleness_min,
              agent_status: agent_status
            )
            AgentCom.TaskQueue.reclaim_task(task.id)
        end
      end
    end)

    Process.send_after(self(), :sweep_stuck, @stuck_sweep_interval_ms)
    {:noreply, state}
end
```

**Hub wake_result handler in socket.ex.**

Add a handler for the `wake_result` message type in `handle_msg/2`. The hub should log the wake result and update the task's progress timestamp.

Add after the `task_recovering` handler (around line 493):

```elixir
defp handle_msg(%{"type" => "wake_result", "task_id" => task_id} = msg, state) do
    status = msg["status"] || "unknown"
    log_task_event(state.agent_id, "wake_result", task_id, msg)

    # Touch task timestamp to prevent premature stuck sweep
    if status == "success" do
      AgentCom.TaskQueue.update_progress(task_id)
    end

    # No reply needed -- informational, hub logs and updates timestamp
    {:ok, state}
end
```

Also add "wake_result" to the validation schema if needed. Check `lib/agent_com/validation.ex` for the WS message type list and add "wake_result" if it's validated there.
  </action>
  <verify>
1. In scheduler.ex, confirm `@stuck_threshold_ms` is 600_000.
2. In scheduler.ex, confirm the sweep_stuck handler calls `AgentCom.AgentFSM.get_state/1` before reclaiming.
3. In scheduler.ex, confirm agent :working gets 2x threshold patience.
4. In socket.ex, confirm a `handle_msg` clause exists for `%{"type" => "wake_result"}`.
5. Run `mix compile --warnings-as-errors` to confirm no compilation errors.
  </verify>
  <done>Stuck sweep uses 10-minute threshold and checks agent online status before reclaiming (offline=immediate, working=2x patience, other=reclaim). Hub handles wake_result messages by logging and updating task progress timestamps.</done>
</task>

</tasks>

<verification>
1. `mix compile --warnings-as-errors` passes with the new socket handler and scheduler changes
2. Execution timeout wraps all executeTask calls in Promise.race
3. Stuck sweep differentiates between offline and online agents
4. Generation check prevents stale task execution
5. wake_result messages are handled by hub without errors
</verification>

<success_criteria>
- PIPE-02: Agentic task exceeding 30-minute deadline is killed and reported as timeout failure
- PIPE-03: Task stuck >10 minutes with offline agent is automatically requeued; working agent gets extended patience
- PIPE-04: Sidecar checks generation before executing, skips stale assignments
- Hub: wake_result messages logged and task progress updated
</success_criteria>

<output>
After completion, create `.planning/phases/39-pipeline-reliability/39-02-SUMMARY.md`
</output>
