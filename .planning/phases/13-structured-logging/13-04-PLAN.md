---
phase: 13-structured-logging
plan: 04
type: execute
wave: 3
depends_on: ["13-02", "13-03"]
files_modified:
  - test/agent_com/log_format_test.exs
  - test/agent_com/telemetry_test.exs
  - sidecar/test/log.test.js
autonomous: true

must_haves:
  truths:
    - "Tests assert that Logger output from GenServers is valid JSON with required fields (time, severity, message)"
    - "Tests assert that telemetry events fire with correct measurements and metadata for key lifecycle points"
    - "Tests assert that sidecar log output is valid JSON with matching field conventions"
    - "Tests assert that auth tokens are redacted in log output"
    - "All new tests pass alongside existing test suite"
  artifacts:
    - path: "test/agent_com/log_format_test.exs"
      provides: "Per-module JSON format assertion tests + redaction tests"
      contains: "Jason.decode"
    - path: "test/agent_com/telemetry_test.exs"
      provides: "Telemetry event emission tests for task, agent, FSM, scheduler, DETS events"
      contains: "telemetry.attach"
    - path: "sidecar/test/log.test.js"
      provides: "Node.js structured logger unit tests"
      contains: "JSON.parse"
  key_links:
    - from: "test/agent_com/log_format_test.exs"
      to: "lib/agent_com/*.ex"
      via: "CaptureLog captures Logger output from GenServer operations"
      pattern: "capture_log"
    - from: "test/agent_com/telemetry_test.exs"
      to: "lib/agent_com/telemetry.ex"
      via: "Attaches test handler, triggers operations, asserts events received"
      pattern: "telemetry.attach"
---

<objective>
Create per-module JSON assertion tests and telemetry event emission tests that verify the structured logging migration is complete and correct.

Purpose: Per user decision, tests must assert log output is valid JSON with required fields (per-module assertion tests, not just smoke). These tests serve as regression guards ensuring no future changes revert to unstructured logging or break telemetry event emission. They also verify secret redaction works.

Output: 2 Elixir test files + 1 Node.js test file covering log format, telemetry events, and sidecar logging
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-structured-logging/13-RESEARCH.md
@.planning/phases/13-structured-logging/13-02-SUMMARY.md
@.planning/phases/13-structured-logging/13-03-SUMMARY.md
@lib/agent_com/telemetry.ex
@test/test_helper.exs
@test/support/
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Elixir log format assertion tests and telemetry event tests</name>
  <files>
    test/agent_com/log_format_test.exs
    test/agent_com/telemetry_test.exs
  </files>
  <action>
    **test/agent_com/log_format_test.exs:**

    Create a test module that verifies Logger output is valid JSON with required fields. Use ExUnit.CaptureLog to capture log output from real operations.

    1. Test that basic Logger.info produces valid JSON:
       ```elixir
       test "logger output is valid JSON with required fields" do
         log_output = capture_log([level: :info], fn ->
           Logger.info("test_event", custom_field: "value")
         end)

         for line <- String.split(log_output, "\n", trim: true) do
           assert {:ok, parsed} = Jason.decode(line)
           assert is_binary(parsed["time"]), "missing time field"
           assert parsed["severity"] in ["debug", "info", "notice", "warning", "error"], "invalid severity"
           assert is_binary(parsed["message"]), "missing message field"
         end
       end
       ```

    2. Test secret redaction:
       ```elixir
       test "auth tokens are redacted in log output" do
         log_output = capture_log([level: :info], fn ->
           Logger.info("auth_event", token: "secret-abc-123")
         end)

         for line <- String.split(log_output, "\n", trim: true) do
           {:ok, parsed} = Jason.decode(line)
           refute parsed["token"] == "secret-abc-123", "token was not redacted"
           if parsed["token"], do: assert parsed["token"] == "[REDACTED]"
         end
       end
       ```

    3. Test that metadata keys (module, agent_id, task_id) appear when set:
       ```elixir
       test "process metadata appears in log output" do
         log_output = capture_log([level: :info], fn ->
           Logger.metadata(module: AgentCom.TaskQueue, agent_id: "test-agent", task_id: "task-123")
           Logger.info("test_with_metadata")
         end)

         line = log_output |> String.split("\n", trim: true) |> List.first()
         {:ok, parsed} = Jason.decode(line)
         assert parsed["module"] =~ "TaskQueue" or parsed["metadata"]["module"] =~ "TaskQueue"
       end
       ```

    4. Test per-module: For 2-3 representative GenServers (e.g., TaskQueue, DetsBackup), trigger a real operation via capture_log and assert the output is valid JSON. Use existing test helpers/factories where available. These don't need full GenServer setup -- just verify that when a GenServer logs, the output is JSON.

    NOTE: config/test.exs sets level: :warning. For these tests, use `capture_log([level: :info], fn -> ... end)` to temporarily capture at lower levels. Or set the capture level in the test setup.

    **test/agent_com/telemetry_test.exs:**

    Create a test module that verifies telemetry events fire correctly.

    1. Test setup: Attach a test telemetry handler that sends events to the test process:
       ```elixir
       setup do
         test_pid = self()
         handler_id = "test-handler-#{inspect(self())}"

         :telemetry.attach_many(
           handler_id,
           [
             [:agent_com, :task, :submit],
             [:agent_com, :task, :assign],
             [:agent_com, :task, :complete],
             [:agent_com, :task, :fail],
             [:agent_com, :fsm, :transition],
             [:agent_com, :agent, :connect],
             [:agent_com, :agent, :disconnect],
             [:agent_com, :agent, :evict],
             [:agent_com, :scheduler, :attempt],
             [:agent_com, :scheduler, :match],
             [:agent_com, :dets, :backup, :start],
             [:agent_com, :dets, :backup, :stop]
           ],
           fn event, measurements, metadata, _config ->
             send(test_pid, {:telemetry_event, event, measurements, metadata})
           end,
           nil
         )

         on_exit(fn -> :telemetry.detach(handler_id) end)
         :ok
       end
       ```

    2. Test telemetry handler attachment:
       ```elixir
       test "AgentCom.Telemetry handlers are attached" do
         handlers = :telemetry.list_handlers([:agent_com, :task, :submit])
         assert length(handlers) >= 1, "no handlers attached for task:submit"
       end
       ```

    3. Test individual telemetry events by directly calling :telemetry.execute (unit test of event shape):
       ```elixir
       test "task:submit event has correct shape" do
         :telemetry.execute(
           [:agent_com, :task, :submit],
           %{queue_depth: 5},
           %{task_id: "test-123", priority: "normal", submitted_by: "agent-1"}
         )

         assert_receive {:telemetry_event, [:agent_com, :task, :submit], measurements, metadata}
         assert measurements.queue_depth == 5
         assert metadata.task_id == "test-123"
       end
       ```

    4. Test FSM transition event shape:
       ```elixir
       test "fsm:transition event carries duration and states" do
         :telemetry.execute(
           [:agent_com, :fsm, :transition],
           %{duration_ms: 150},
           %{agent_id: "agent-1", from_state: :idle, to_state: :assigned, task_id: "task-1"}
         )

         assert_receive {:telemetry_event, [:agent_com, :fsm, :transition], measurements, metadata}
         assert measurements.duration_ms == 150
         assert metadata.from_state == :idle
         assert metadata.to_state == :assigned
       end
       ```

    5. Optionally, if test infrastructure allows starting a GenServer: Submit a real task via TaskQueue and assert the :submit telemetry event fires. Use existing test helpers from Phase 9. But keep this optional -- the direct :telemetry.execute tests are the primary verification.

    IMPORTANT: Each test must be independent. Clean up telemetry handlers in on_exit. Do NOT leave stale handlers that affect other tests.
  </action>
  <verify>
    Run `mix test test/agent_com/log_format_test.exs test/agent_com/telemetry_test.exs` -- all new tests pass.
    Run `mix test` -- full test suite passes (no regressions).
  </verify>
  <done>
    log_format_test.exs: Tests verify Logger output is valid JSON, required fields present, metadata propagation works, auth tokens are redacted.
    telemetry_test.exs: Tests verify handler attachment, event shapes for task/FSM/agent/scheduler/DETS events, correct measurements and metadata.
    All tests pass alongside existing test suite.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create sidecar structured logger tests</name>
  <files>
    sidecar/test/log.test.js
  </files>
  <action>
    Create sidecar/test/log.test.js using Node.js built-in test runner (node:test) per project convention from Phase 9.

    1. Test that log output is valid JSON with required fields:
       ```javascript
       const { describe, it, beforeEach } = require('node:test');
       const assert = require('node:assert/strict');
       const { initLogger, log, LEVELS } = require('../lib/log');

       describe('Structured Logger', () => {
         beforeEach(() => {
           initLogger({ agent_id: 'test-agent' });
         });

         it('outputs valid JSON to stdout', (t) => {
           // Capture stdout
           let captured = '';
           const originalWrite = process.stdout.write;
           process.stdout.write = (chunk) => { captured += chunk; };

           log('info', 'test_event', { key: 'value' });

           process.stdout.write = originalWrite;

           const parsed = JSON.parse(captured.trim());
           assert.ok(parsed.time, 'missing time field');
           assert.equal(parsed.severity, 'info');
           assert.equal(parsed.message, 'test_event');
           assert.equal(parsed.agent_id, 'test-agent');
           assert.ok(parsed.pid, 'missing pid');
           assert.ok(parsed.node, 'missing node');
           assert.equal(parsed.module, 'sidecar/index'); // default module
           assert.equal(parsed.key, 'value');
         });
       });
       ```

    2. Test secret redaction:
       ```javascript
       it('redacts auth tokens', () => {
         let captured = '';
         const originalWrite = process.stdout.write;
         process.stdout.write = (chunk) => { captured += chunk; };

         log('info', 'auth_test', { token: 'secret-abc-123' });

         process.stdout.write = originalWrite;

         const parsed = JSON.parse(captured.trim());
         assert.equal(parsed.token, '[REDACTED]');
       });
       ```

    3. Test level filtering:
       ```javascript
       it('filters messages below configured level', () => {
         initLogger({ agent_id: 'test', log_level: 'warning' });

         let captured = '';
         const originalWrite = process.stdout.write;
         process.stdout.write = (chunk) => { captured += chunk; };

         log('debug', 'should_not_appear');
         log('info', 'should_not_appear');
         log('warning', 'should_appear');

         process.stdout.write = originalWrite;

         const lines = captured.trim().split('\n').filter(Boolean);
         assert.equal(lines.length, 1);
         assert.equal(JSON.parse(lines[0]).severity, 'warning');
       });
       ```

    4. Test custom module name:
       ```javascript
       it('accepts custom module name', () => {
         let captured = '';
         const originalWrite = process.stdout.write;
         process.stdout.write = (chunk) => { captured += chunk; };

         log('info', 'test', {}, 'sidecar/queue');

         process.stdout.write = originalWrite;

         const parsed = JSON.parse(captured.trim());
         assert.equal(parsed.module, 'sidecar/queue');
       });
       ```

    5. Test field convention parity with Elixir hub (same field names: time, severity, message, module, pid, node, agent_id).

    Ensure the test file is picked up by the existing sidecar test configuration (check package.json test script and how other test files in sidecar/test/ are structured).
  </action>
  <verify>
    Run `cd sidecar && npm test` -- all sidecar tests pass including new log tests.
  </verify>
  <done>
    sidecar/test/log.test.js: Tests verify JSON output format, required fields, secret redaction, level filtering, custom module names, and field convention parity with Elixir hub.
    All sidecar tests pass.
  </done>
</task>

</tasks>

<verification>
- `mix test` -- full Elixir test suite passes (existing + new)
- `cd sidecar && npm test` -- full sidecar test suite passes (existing + new)
- New test files specifically verify: JSON format, required fields, metadata, redaction, telemetry events, sidecar parity
</verification>

<success_criteria>
1. log_format_test.exs passes: JSON format valid, required fields present, metadata propagation, token redaction
2. telemetry_test.exs passes: handler attachment verified, event shapes correct for all lifecycle points
3. sidecar log.test.js passes: JSON format, required fields, redaction, level filtering, field parity
4. Full test suite (mix test + npm test) passes with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/13-structured-logging/13-04-SUMMARY.md`
</output>
