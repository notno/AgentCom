---
phase: 20-sidecar-execution
plan: 03
type: execute
wave: 2
depends_on: ["20-01", "20-02"]
files_modified:
  - sidecar/lib/execution/dispatcher.js
  - sidecar/lib/execution/ollama-executor.js
  - sidecar/lib/execution/claude-executor.js
  - sidecar/lib/execution/shell-executor.js
  - sidecar/index.js
autonomous: true

must_haves:
  truths:
    - "A task with routing_decision.target_type 'ollama' calls the specified Ollama endpoint and returns the model's response with token counts"
    - "A task with routing_decision.target_type 'claude' invokes Claude Code CLI and returns the response with token counts"
    - "A task with routing_decision.target_type 'sidecar' executes the shell command and returns stdout with zero tokens"
    - "Ollama failures retry 1-2 times locally then report failure to hub"
    - "Claude API failures retry 2-3 times with exponential backoff then mark failed"
    - "Trivial shell failures retry once then report failure"
    - "All retry attempts and failures stream to dashboard in real-time via execution_event progress messages"
    - "Every completed task result includes model_used, tokens_in, tokens_out, estimated_cost_usd"
  artifacts:
    - path: "sidecar/lib/execution/dispatcher.js"
      provides: "dispatch function routing to correct executor based on routing_decision"
      exports: ["dispatch"]
    - path: "sidecar/lib/execution/ollama-executor.js"
      provides: "OllamaExecutor with streaming NDJSON, retry 1-2 times, token collection"
      exports: ["OllamaExecutor"]
    - path: "sidecar/lib/execution/claude-executor.js"
      provides: "ClaudeExecutor invoking claude -p with stream-json, retry 2-3 times with backoff"
      exports: ["ClaudeExecutor"]
    - path: "sidecar/lib/execution/shell-executor.js"
      provides: "ShellExecutor with configurable timeout, streaming stdout/stderr, retry once"
      exports: ["ShellExecutor"]
    - path: "sidecar/index.js"
      provides: "Conditional execution dispatch in handleTaskAssign when routing_decision present"
      contains: "executeTask"
  key_links:
    - from: "sidecar/lib/execution/dispatcher.js"
      to: "sidecar/lib/execution/ollama-executor.js"
      via: "require and dispatch based on target_type"
      pattern: "OllamaExecutor.*execute"
    - from: "sidecar/lib/execution/dispatcher.js"
      to: "sidecar/lib/execution/cost-calculator.js"
      via: "calculateCost called after executor completes"
      pattern: "calculateCost"
    - from: "sidecar/index.js"
      to: "sidecar/lib/execution/dispatcher.js"
      via: "executeTask calls dispatch"
      pattern: "dispatch\\(task"
    - from: "sidecar/lib/execution/ollama-executor.js"
      to: "Ollama /api/chat"
      via: "Node.js http.request with streaming NDJSON"
      pattern: "/api/chat"
---

<objective>
Build the three executor modules, the dispatcher, and integrate into the sidecar task lifecycle -- the core execution engine that makes sidecars actually execute tasks.

Purpose: This is the heart of Phase 20. When a task arrives with a routing_decision from the hub, the sidecar dispatches to the correct executor (Ollama HTTP, Claude Code CLI, or shell), streams progress events back to the hub, handles failures with per-tier retry logic (per locked decisions), and reports completion with execution metadata including cost. Falls back to legacy wakeAgent when no routing_decision is present (backward compatibility).

Output: Sidecar can execute all three task types with streaming, retries, and cost reporting.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/20-sidecar-execution/20-RESEARCH.md
@.planning/phases/20-sidecar-execution/20-01-SUMMARY.md
@sidecar/index.js
@sidecar/lib/wake.js
@sidecar/lib/resources.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Dispatcher, OllamaExecutor, and ShellExecutor</name>
  <files>
    sidecar/lib/execution/dispatcher.js
    sidecar/lib/execution/ollama-executor.js
    sidecar/lib/execution/shell-executor.js
  </files>
  <action>
    **dispatcher.js:**
    Create `dispatch(task, config, onProgress)` that reads `task.routing_decision.target_type` and routes to the correct executor. Each executor returns a common `ExecutionResult` shape:
    ```javascript
    {
      status: 'success' | 'failed',
      output: string,
      model_used: string | 'none',
      tokens_in: number,
      tokens_out: number,
      estimated_cost_usd: number,
      equivalent_claude_cost_usd: number | null,
      execution_ms: number,
      error: string | null
    }
    ```
    Use `calculateCost` from `./cost-calculator.js` (Plan 01) to compute cost fields after executor completes. Use `ProgressEmitter` from `./progress-emitter.js` (Plan 01) to batch progress events.

    If `routing_decision` is missing, use fallback inference from `task.complexity.effective_tier` (trivial->sidecar, complex->claude, default->ollama).

    **ollama-executor.js:**
    Create `OllamaExecutor` class with `async execute(task, config, onProgress)`:
    1. Parse endpoint from `task.routing_decision.selected_endpoint` (format: "host:port") or fall back to `config.ollama_url`
    2. Build messages array: system prompt from task context, user message from task description
    3. POST to `/api/chat` with `stream: true` using Node.js built-in `http` module (pattern from resources.js fetchOllamaPs)
    4. Parse streaming NDJSON response line-by-line using buffer + newline split pattern (per research pitfall #3: buffer incomplete lines across chunks)
    5. Emit token progress events via onProgress for each `message.content` chunk
    6. Collect final response when `done: true` -- extract `prompt_eval_count` (input tokens) and `eval_count` (output tokens)
    7. Default `prompt_eval_count` to 0 when absent (per research pitfall #1: Ollama caches prompt evaluations)
    8. 5-minute timeout per generation (long generations)

    **Retry logic (per locked decision):** Retry 1-2 times locally on failure. On each retry, emit status event: `{ type: 'status', message: 'Ollama retry 1/2...' }`. If all retries exhausted, return failed result -- the hub will reassign to a different endpoint.

    **shell-executor.js:**
    Create `ShellExecutor` class with `async execute(task, config, onProgress)`:
    1. Extract command from `task.metadata.shell_command` or `task.description` (trivial tasks carry the command)
    2. Use `child_process.spawn` (NOT exec) for streaming stdout/stderr (per research: exec buffers, spawn streams)
    3. Stream stdout as `{ type: 'stdout', text }` events and stderr as `{ type: 'stderr', text }` events (per discretion decision: separate streams)
    4. Configurable timeout from `task.metadata.timeout_ms` or default 60s (per locked decision: configurable per-task)
    5. On timeout: SIGTERM then SIGKILL after 5s grace period (per research pitfall #4: zombie processes)
    6. Return `{ status, output: stdout, model_used: 'none', tokens_in: 0, tokens_out: 0, ... }`

    **Retry logic (per locked decision):** Retry once on failure. Emit status event: `{ type: 'status', message: 'Shell retry 1/1...' }`.

    No additional sandboxing beyond process-level isolation (per discretion decision).
  </action>
  <verify>
    `node -e "require('./sidecar/lib/execution/dispatcher')"` -- loads without error.
    `node -e "require('./sidecar/lib/execution/ollama-executor')"` -- loads without error.
    `node -e "require('./sidecar/lib/execution/shell-executor')"` -- loads without error.
    Verify ShellExecutor can execute a simple command: create a quick smoke test script that runs `echo hello` via ShellExecutor.
  </verify>
  <done>Dispatcher routes tasks by target_type. OllamaExecutor streams NDJSON from /api/chat with retry 1-2x. ShellExecutor runs commands via spawn with streaming stdout/stderr, configurable timeout, and retry 1x. All return ExecutionResult with cost from cost-calculator.</done>
</task>

<task type="auto">
  <name>Task 2: ClaudeExecutor and index.js integration</name>
  <files>
    sidecar/lib/execution/claude-executor.js
    sidecar/index.js
  </files>
  <action>
    **claude-executor.js:**
    Create `ClaudeExecutor` class with `async execute(task, config, onProgress)`:
    1. Build prompt from task description, context (repo, branch, file_hints, success_criteria)
    2. Invoke `claude -p "{prompt}" --output-format stream-json --verbose` via `child_process.spawn` (per locked decision: Claude API calls go through Claude Code)
    3. Parse streaming JSON events from stdout line-by-line:
       - `stream_event` with `delta.type === 'text_delta'`: emit token progress event with delta.text
       - `result` event: capture usage stats (usage.input_tokens, usage.output_tokens)
    4. Capture stderr as status events (Claude Code writes status info to stderr)
    5. Handle missing usage data defensively: report tokens_in/tokens_out as 0 when absent (per research pitfall #5)
    6. Build prompt with task context: include description, success_criteria, file_hints for Claude to work with

    **Retry logic (per locked decision):** Retry 2-3 times with exponential backoff (2s, 4s, 8s). On each retry, emit status event: `{ type: 'status', message: 'Claude retry 1/3, backing off 2s...' }`. No downgrade to lesser model -- complex tasks need Claude. On all retries exhausted, return failed result.

    Check for `claude` CLI availability at execution time. If ENOENT, return clear failure: "Claude Code CLI not found -- install via npm install -g @anthropic-ai/claude-code" (per research pitfall #2).

    **index.js integration:**
    In `handleTaskAssign(msg)`, after the existing task object creation, store routing_decision from the message:
    ```javascript
    // Store routing decision (Phase 20)
    routing_decision: msg.routing_decision || null,
    ```

    After the git workflow section (line ~612), replace the unconditional `wakeAgent(task, this)` with conditional execution dispatch:
    ```javascript
    // Phase 20: Direct execution when routing_decision present
    const routing = task.routing_decision;
    if (routing && routing.target_type && routing.target_type !== 'wake') {
      this.executeTask(task);
    } else {
      // Legacy: wake agent and watch for result file
      wakeAgent(task, this);
    }
    ```

    Add `executeTask(task)` method on HubConnection class:
    ```javascript
    async executeTask(task) {
      const { dispatch } = require('./lib/execution/dispatcher');
      const { ProgressEmitter } = require('./lib/execution/progress-emitter');

      const emitter = new ProgressEmitter((event) => {
        this.send({
          type: 'task_progress',
          task_id: task.task_id,
          execution_event: {
            event_type: event.type,
            text: event.text || event.message || '',
            tokens_so_far: event.tokens_so_far || null,
            model: event.model || null,
            timestamp: Date.now()
          }
        });
      }, { batchIntervalMs: 100 });

      try {
        task.status = 'working';
        saveQueue(QUEUE_PATH, _queue);

        const result = await dispatch(task, _config, (event) => emitter.emit(event));
        emitter.flush();
        emitter.destroy();

        this.sendTaskComplete(task.task_id, {
          status: result.status,
          output: result.output,
          model_used: result.model_used,
          tokens_in: result.tokens_in,
          tokens_out: result.tokens_out,
          estimated_cost_usd: result.estimated_cost_usd,
          equivalent_claude_cost_usd: result.equivalent_claude_cost_usd,
          execution_ms: result.execution_ms
        });
      } catch (err) {
        emitter.destroy();
        log('error', 'execution_failed', { task_id: task.task_id, error: err.message });
        this.sendTaskFailed(task.task_id, err.message);
      }

      _queue.active = null;
      saveQueue(QUEUE_PATH, _queue);
    }
    ```

    Also add `routing_decision` to the require list at top of index.js: no new requires needed (dispatcher/emitter are lazy-loaded inside executeTask).
  </action>
  <verify>
    `node -e "require('./sidecar/lib/execution/claude-executor')"` -- loads without error.
    `node -c sidecar/index.js` -- syntax check passes.
    Verify the executeTask method exists on HubConnection prototype by reading the compiled source.
    ShellExecutor smoke test from Task 1 should still work.
  </verify>
  <done>ClaudeExecutor invokes Claude Code CLI with stream-json, retries 2-3x with exponential backoff, handles missing usage data. index.js conditionally dispatches to execution engine when routing_decision present, falls back to wakeAgent for legacy tasks. Progress events stream to hub via ProgressEmitter with 100ms batching.</done>
</task>

</tasks>

<verification>
- All four execution modules load without error via `node -e "require(...)"`
- `node -c sidecar/index.js` passes syntax check
- ShellExecutor can execute `echo hello` and return output
- index.js handleTaskAssign routes to executeTask when routing_decision has target_type
- index.js handleTaskAssign still calls wakeAgent when no routing_decision (backward compat)
- Each executor includes retry logic matching locked decisions (Ollama: 1-2x, Claude: 2-3x backoff, Shell: 1x)
- Each executor emits progress events via onProgress callback
- ExecutionResult includes model_used, tokens_in, tokens_out, estimated_cost_usd fields
</verification>

<success_criteria>
1. OllamaExecutor calls /api/chat with streaming NDJSON and collects token counts
2. ClaudeExecutor invokes claude -p with stream-json and captures usage statistics
3. ShellExecutor runs commands via spawn with configurable timeout and zero LLM tokens
4. Each executor follows locked retry policy and streams retry events in real-time
5. Dispatcher routes correctly based on routing_decision.target_type
6. index.js conditionally dispatches to execution engine vs legacy wake path
7. All completed results include model_used, tokens_in, tokens_out, estimated_cost_usd, equivalent_claude_cost_usd
</success_criteria>

<output>
After completion, create `.planning/phases/20-sidecar-execution/20-03-SUMMARY.md`
</output>
