---
phase: 20-sidecar-execution
plan: 04
type: execute
wave: 3
depends_on: ["20-02", "20-03"]
files_modified:
  - lib/agent_com/dashboard_state.ex
  - lib/agent_com/dashboard_socket.ex
  - priv/static/index.html
autonomous: false

must_haves:
  truths:
    - "Dashboard shows real-time streaming execution output (tokens, stdout, status messages) as tasks execute"
    - "Dashboard shows per-task cost breakdown (model used, tokens in/out, estimated cost, equivalent Claude cost for Ollama tasks)"
    - "Retry and failure events appear in real-time on the dashboard during task execution"
  artifacts:
    - path: "lib/agent_com/dashboard_socket.ex"
      provides: "Handles execution_progress events from PubSub and pushes to dashboard WebSocket"
      contains: "execution_progress"
    - path: "priv/static/index.html"
      provides: "Execution output panel and cost display in task detail"
      contains: "execution-output"
  key_links:
    - from: "lib/agent_com/dashboard_socket.ex"
      to: "priv/static/index.html"
      via: "WebSocket push of execution_event for dashboard rendering"
      pattern: "execution_event"
    - from: "lib/agent_com/dashboard_state.ex"
      to: "lib/agent_com/dashboard_socket.ex"
      via: "PubSub tasks topic subscription"
      pattern: "task_event.*execution_progress"
---

<objective>
Dashboard rendering of real-time execution streaming and per-task cost visibility -- making the execution pipeline observable from the UI.

Purpose: Per locked decisions, LLM response tokens and trivial task stdout stream to the dashboard in real-time, retry/failure events are visible as they happen, and completed tasks show cost breakdown (model, tokens, cost in USD, equivalent Claude cost for Ollama tasks). This plan connects the PubSub events (wired in Plan 02) to the dashboard frontend.

Output: Dashboard shows live execution output and cost metrics for all task types.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/20-sidecar-execution/20-02-SUMMARY.md
@.planning/phases/20-sidecar-execution/20-03-SUMMARY.md
@lib/agent_com/dashboard_socket.ex
@priv/static/index.html
</context>

<tasks>

<task type="auto">
  <name>Task 1: DashboardSocket execution event handler and DashboardState cost tracking</name>
  <files>
    lib/agent_com/dashboard_socket.ex
    lib/agent_com/dashboard_state.ex
  </files>
  <action>
    **DashboardSocket:**
    The DashboardSocket already subscribes to PubSub "tasks" topic. Add handling for the new `:execution_progress` event type (emitted by Socket.ex in Plan 02):

    ```elixir
    def handle_info({:task_event, %{event: :execution_progress, task_id: task_id, execution_event: event}}, state) do
      push = %{
        "type" => "execution_event",
        "task_id" => task_id,
        "event_type" => event["event_type"] || Map.get(event, :event_type),
        "text" => event["text"] || Map.get(event, :text, ""),
        "tokens_so_far" => event["tokens_so_far"] || Map.get(event, :tokens_so_far),
        "model" => event["model"] || Map.get(event, :model),
        "timestamp" => event["timestamp"] || Map.get(event, :timestamp)
      }
      {:push, {:text, Jason.encode!(push)}, state}
    end
    ```

    This uses the DashboardSocket's existing batching -- the `handle_info` pushes directly (DashboardSocket already batches at 100ms per its existing implementation). No additional batching needed since the sidecar's ProgressEmitter already batches at 100ms.

    **DashboardState:**
    When a task_complete event arrives with execution metadata (model_used, tokens_in, tokens_out, estimated_cost_usd, equivalent_claude_cost_usd), store these in the task's state for the dashboard snapshot endpoint. The dashboard already reads task state via the snapshot API. Ensure execution metadata fields are preserved when updating task state from task_complete events.

    Check existing DashboardState to see how task completion is handled. If it reads from TaskQueue, the metadata is already stored there (Plan 02 passes it through). If DashboardState maintains its own state, add the execution fields to whatever structure it uses for completed tasks.
  </action>
  <verify>Run `mix compile` -- no errors. Run `mix test test/agent_com/dashboard*` -- all existing dashboard tests pass.</verify>
  <done>DashboardSocket forwards execution_progress events to dashboard WebSocket. DashboardState preserves execution metadata for completed tasks. No regressions.</done>
</task>

<task type="auto">
  <name>Task 2: Dashboard frontend execution output panel and cost display</name>
  <files>
    priv/static/index.html
  </files>
  <action>
    Add to the dashboard HTML/JS:

    **1. Execution Output Panel:**
    Add a collapsible panel below the task detail area that shows streaming execution output. When a task is selected/active and execution_event messages arrive via WebSocket:
    - `token` events: append text to a pre/code block (monospace, scrolls to bottom)
    - `stdout` events: append text in default color
    - `stderr` events: append text in amber/warning color
    - `status` events: append as italicized status line (e.g., "Ollama retry 1/2...")
    - `error` events: append in red/error color
    - Auto-scroll to bottom as new content arrives
    - Clear panel when a new task starts executing

    **2. Cost Display:**
    In the task detail/completion area, show execution metadata when available:
    - Model used (e.g., "llama3.2:latest", "claude-sonnet-4.5", "none")
    - Tokens: input / output count
    - Cost: $X.XXXX (or "$0.00 (local)" for Ollama)
    - If equivalent_claude_cost_usd is present, show "Saved: $X.XX vs Claude" in green text
    - For trivial tasks: show "0 tokens, $0.00 (shell execution)"

    **3. WebSocket handler:**
    In the dashboard WebSocket message handler, add case for `type: 'execution_event'`:
    ```javascript
    case 'execution_event':
      renderExecutionEvent(msg);
      break;
    ```

    And on `task_complete` / task detail display, render cost fields if present in the task result.

    **Style:** Match existing dashboard styling (existing CSS variables, card patterns, etc). Use table layout for cost breakdown matching the existing table patterns (per locked decision from Phase 18: table view for data).
  </action>
  <verify>Open dashboard in browser, verify execution output panel renders without JavaScript errors. Check that the panel is visible in the task detail area.</verify>
  <done>Dashboard shows execution output panel with streaming token/stdout/status/error display. Cost breakdown shows model, tokens, cost, and equivalent Claude savings for Ollama tasks. All execution event types render with appropriate styling.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Visual verification of execution streaming and cost display</name>
  <files>priv/static/index.html</files>
  <action>
    Human verification of dashboard visual output. No code changes -- this is a visual check of Tasks 1 and 2 output.

    **What was built:** Dashboard execution output panel showing real-time streaming from all three executor types, and per-task cost breakdown with equivalent Claude savings display.

    **How to verify:**
    1. Start the hub: `mix run --no-halt`
    2. Open the dashboard at http://localhost:4000
    3. Navigate to the task detail area
    4. Verify the execution output panel is visible (even if empty when no tasks are running)
    5. Check that the cost display area exists in task details
    6. If a test task can be submitted: verify streaming output appears in real-time
    7. Verify retry/status messages would appear distinctly styled (italicized status lines)
    8. Confirm no JavaScript console errors
  </action>
  <verify>Visual inspection confirms execution output panel and cost display are present and functional.</verify>
  <done>Dashboard execution streaming and cost display verified visually. Type "approved" or describe issues to fix.</done>
</task>

</tasks>

<verification>
- `mix compile` succeeds with no warnings related to dashboard modules
- `mix test test/agent_com/dashboard*` -- all existing tests pass
- Dashboard loads without JavaScript errors
- execution_event WebSocket messages are handled by DashboardSocket
- Execution output panel visible in task detail area
- Cost display renders model, tokens, and cost when data available
</verification>

<success_criteria>
1. Real-time execution output streams to dashboard as tasks execute (tokens, stdout, status events)
2. Per-task cost breakdown visible (model, tokens in/out, cost USD)
3. Ollama tasks show equivalent Claude cost demonstrating savings
4. Retry and failure events appear in real-time with distinct styling
5. Visual verification approved by user
</success_criteria>

<output>
After completion, create `.planning/phases/20-sidecar-execution/20-04-SUMMARY.md`
</output>
