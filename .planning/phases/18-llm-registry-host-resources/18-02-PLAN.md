---
phase: 18-llm-registry-host-resources
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - sidecar/lib/resources.js
  - sidecar/index.js
autonomous: true

must_haves:
  truths:
    - "Sidecar collects CPU percent, RAM used/total, and VRAM data from local Ollama"
    - "Sidecar sends resource_report WS message every 30 seconds with collected metrics"
    - "Sidecar includes ollama_url in identify message if configured"
    - "Resource collection gracefully handles missing GPU/Ollama (reports null for unavailable metrics)"
  artifacts:
    - path: "sidecar/lib/resources.js"
      provides: "Resource metrics collection module using os built-in and Ollama /api/ps"
      exports: ["collectMetrics"]
    - path: "sidecar/index.js"
      provides: "Extended identify message with ollama_url, periodic resource_report sending"
  key_links:
    - from: "sidecar/lib/resources.js"
      to: "os module"
      via: "Node.js os.cpus(), os.totalmem(), os.freemem()"
      pattern: "os\\.(cpus|totalmem|freemem)"
    - from: "sidecar/lib/resources.js"
      to: "Ollama /api/ps"
      via: "HTTP GET for VRAM data"
      pattern: "fetch.*api/ps|http.*api/ps"
    - from: "sidecar/index.js"
      to: "sidecar/lib/resources.js"
      via: "require and periodic call"
      pattern: "require.*resources"
---

<objective>
Add resource metrics collection to the sidecar and extend the identify/heartbeat protocol to report Ollama URL and host resource utilization to the hub.

Purpose: Sidecars are the only component with local access to host CPU/RAM/GPU data. This plan makes them report that data so the hub can display it on the dashboard.
Output: `resources.js` module and updated `index.js` with `ollama_url` in identify and periodic `resource_report` messages.
</objective>

<execution_context>
@C:/Users/nrosq/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nrosq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/18-llm-registry-host-resources/18-CONTEXT.md
@.planning/phases/18-llm-registry-host-resources/18-RESEARCH.md

@sidecar/index.js (full sidecar codebase)
@sidecar/lib/log.js (structured logging pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create resources.js metrics collection module</name>
  <files>sidecar/lib/resources.js</files>
  <action>
    Create `sidecar/lib/resources.js` that exports a single async function `collectMetrics(ollamaUrl)`.

    **CPU percent:**
    - Use `os.cpus()` to get per-core times, compute average utilization across all cores.
    - Take two snapshots 1 second apart (or use cached previous snapshot for non-blocking approach).
    - For simplicity in the first version: compute instantaneous CPU from `os.loadavg()[0]` divided by `os.cpus().length * 100` capped at 100. This gives a 1-minute load average as percent.
    - Actually, simpler: use `os.loadavg()[0] / os.cpus().length * 100` capped at 100.0.

    **RAM:**
    - `ram_total_bytes`: `os.totalmem()`
    - `ram_used_bytes`: `os.totalmem() - os.freemem()`

    **VRAM (optional, from Ollama /api/ps):**
    - If `ollamaUrl` is provided, HTTP GET `{ollamaUrl}/api/ps`.
    - Parse response JSON. The response has `models` array; each model has `size_vram` (bytes loaded in VRAM) and `size` (total model size).
    - Sum `size_vram` across all loaded models for `vram_used_bytes`.
    - For `vram_total_bytes`: not directly available from Ollama API, so report as `null`. The used bytes from /api/ps is the meaningful metric per HOST-02.
    - If Ollama is unreachable or /api/ps returns error, set vram fields to `null`.
    - Use Node.js built-in `http` or `https` module (NOT fetch, to avoid Node version dependency). Use the same pattern as existing code.

    **Return value:**
    ```js
    {
      cpu_percent: 45.2,           // float, 0.0-100.0
      ram_used_bytes: 8589934592,  // integer
      ram_total_bytes: 17179869184, // integer
      vram_used_bytes: 4294967296, // integer or null
      vram_total_bytes: null       // always null (not available from Ollama)
    }
    ```

    **Error handling:** Never throw. If any metric fails, use null for that field. Log warnings via the structured logger from `lib/log.js`.

    Use `'use strict';` at top. Use `const os = require('os');` and `const http = require('http');`. No new npm dependencies.
  </action>
  <verify>
    `node -e "const r = require('./sidecar/lib/resources'); r.collectMetrics(null).then(m => { console.log(JSON.stringify(m)); process.exit(m.cpu_percent !== null && m.ram_total_bytes > 0 ? 0 : 1); })"` exits 0 with valid JSON containing cpu_percent and ram fields.
  </verify>
  <done>
    `collectMetrics()` returns an object with `cpu_percent` (float), `ram_used_bytes`, `ram_total_bytes` (integers), and `vram_used_bytes`/`vram_total_bytes` (integer or null). Works without Ollama installed (VRAM fields are null).
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend sidecar identify and add periodic resource reporting</name>
  <files>sidecar/index.js</files>
  <action>
    Modify the `HubConnection` class in `sidecar/index.js`:

    **1. Add ollama_url to config loading:**
    - In `loadConfig()`, add optional field: `config.ollama_url = config.ollama_url || null;`
    - This allows users to set `"ollama_url": "http://localhost:11434"` in config.json.

    **2. Extend identify message:**
    - In the `identify()` method, add `ollama_url: this.config.ollama_url` to the identify payload (alongside existing agent_id, token, name, etc.).
    - Only include the field if it's truthy (don't send null).

    **3. Add periodic resource reporting:**
    - Require resources.js at the top: `const { collectMetrics } = require('./lib/resources');`
    - Add a `startResourceReporting()` method that sets up a 30-second interval:
      ```js
      startResourceReporting() {
        if (this.resourceInterval) clearInterval(this.resourceInterval);
        this.resourceInterval = setInterval(async () => {
          if (!this.identified) return;
          const metrics = await collectMetrics(this.config.ollama_url);
          this.send({
            type: 'resource_report',
            agent_id: this.config.agent_id,
            ...metrics,
            timestamp: Date.now()
          });
        }, 30000);
      }
      ```
    - Call `this.startResourceReporting()` right after `this.identify()` in the `open` event handler (line ~362).
    - Clear the interval in `cleanup()` method: `if (this.resourceInterval) clearInterval(this.resourceInterval);`

    **4. Also send an initial resource_report shortly after identify (5 seconds delay):**
    - In the `open` handler, add: `setTimeout(() => this.sendResourceReport(), 5000);`
    - Extract the metric collection + send into a `sendResourceReport()` method for reuse.

    **Do NOT modify any existing message handling or task protocol logic.** Only add the new resource reporting alongside existing functionality.
  </action>
  <verify>
    `node -c sidecar/index.js` -- syntax check passes. Verify `require('./lib/resources')` is present. Verify identify payload includes `ollama_url` conditional. Verify resource_report interval setup in open handler.
  </verify>
  <done>
    Sidecar sends `ollama_url` in identify message (when configured), starts a 30-second resource_report interval after connecting, and sends an initial report 5 seconds after identify. Resource interval is cleaned up on disconnect/shutdown.
  </done>
</task>

</tasks>

<verification>
- `node -c sidecar/index.js` -- no syntax errors
- `node -e "const r = require('./sidecar/lib/resources'); r.collectMetrics(null).then(console.log)"` -- returns valid metrics object
- Grep for `resource_report` in sidecar/index.js confirms message sending
- Grep for `ollama_url` in sidecar/index.js confirm identify extension
</verification>

<success_criteria>
- `resources.js` exists and exports `collectMetrics` that returns CPU/RAM/VRAM data
- `index.js` sends `ollama_url` in identify when configured
- `index.js` sends periodic `resource_report` messages every 30 seconds
- No new npm dependencies added
- Existing sidecar functionality unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/18-llm-registry-host-resources/18-02-SUMMARY.md`
</output>
